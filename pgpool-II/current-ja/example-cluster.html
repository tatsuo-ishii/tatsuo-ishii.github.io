<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Pgpool-II + Watchdogの構築の例</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:pgsql-docs@postgresql.org"><LINK
REL="HOME"
TITLE="Pgpool-II 4.1devel 文書"
HREF="index.html"><LINK
REL="UP"
TITLE="設定の例"
HREF="example-configs.html"><LINK
REL="PREVIOUS"
TITLE="Watchdogの設定例"
HREF="example-watchdog.html"><LINK
REL="NEXT"
TITLE="AWS設定の例"
HREF="example-aws.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stylesheet.css"><META
HTTP-EQUIV="Content-Type"
CONTENT="text/html; charset=utf-8"><META
NAME="creation"
CONTENT="2019-09-05T11:44:25"></HEAD
><BODY
CLASS="SECT1"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="4"
ALIGN="center"
VALIGN="bottom"
><A
HREF="index.html"
>Pgpool-II 4.1devel 文書</A
></TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
TITLE="Watchdogの設定例"
HREF="example-watchdog.html"
ACCESSKEY="P"
>前のページ</A
></TD
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>上に戻る</A
></TD
><TD
WIDTH="60%"
ALIGN="center"
VALIGN="bottom"
>第 8章設定の例</TD
><TD
WIDTH="20%"
ALIGN="right"
VALIGN="top"
><A
TITLE="AWS設定の例"
HREF="example-aws.html"
ACCESSKEY="N"
>次のページ</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="EXAMPLE-CLUSTER"
>8.3. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> + Watchdogの構築の例</A
></H1
><P
>  ここでは、ストリーミングレプリケーション構成の<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>を<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>で管理するシステムの構成例を示します。この例では、3台の<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を使って<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>を管理し、単一障害点やスプリットブレインの起きない堅牢なクラスタを運用することが可能です。
 </P
><P
>  この設定例では<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 11を使っていますが、各種スクリプトは<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 12でも動作確認を行っています。
 </P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-REQUIREMENT"
>8.3.1. 前提条件</A
></H2
><P
>   <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>サーバと<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>サーバが同じサブネットにあることを前提とします。
  </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-STRUCTURE"
>8.3.2. 全体構成</A
></H2
><P
>   今回は、Linuxサーバを3台用意し、それぞれのホスト名は 「server1」、「server2」、「server3」 とします。使用するOSはすべてCentOS 7.4とします。それぞれのサーバに<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>と<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>をインストールします。3台の<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>がストリーミングレプリケーション構成になります。全体構成図は以下の通りです。
  </P
><P
>   <DIV
CLASS="FIGURE"
><A
NAME="AEN5983"
></A
><P
><B
>図 8-1. 全体構成図</B
></P
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG
SRC="cluster_40.gif"></P
></DIV
></DIV
>
  </P
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>注意: </B
>    「アクティブ」「スタンバイ」「Primary」「Standby」といった役割は固定されているものではなく、運用と共に変化することがあります。
   </P
></BLOCKQUOTE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-IP"
></A
><P
><B
>表 8-2. ホスト名とIPアドレス</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>ホスト名</TH
><TH
>IPアドバイス</TH
><TH
>仮想IP</TH
></TR
></THEAD
><TBODY
><TR
><TD
>server1</TD
><TD
>192.168.137.101</TD
><TD
ROWSPAN="3"
>192.168.137.150</TD
></TR
><TR
><TD
>server2</TD
><TD
>192.168.137.102</TD
></TR
><TR
><TD
>server3</TD
><TD
>192.168.137.103</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-POSTGRESQL-CONFIG"
></A
><P
><B
>表 8-3. PostgreSQLのバージョンと設定情報</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>項目</TH
><TH
>値</TH
><TH
>説明</TH
></TR
></THEAD
><TBODY
><TR
><TD
>PostgreSQLバージョン</TD
><TD
>11.1</TD
><TD
>-</TD
></TR
><TR
><TD
>ポート番号</TD
><TD
>5432</TD
><TD
>-</TD
></TR
><TR
><TD
>$PGDATA</TD
><TD
>/var/lib/pgsql/11/data</TD
><TD
>-</TD
></TR
><TR
><TD
>アーカイブモード</TD
><TD
>有効</TD
><TD
>/var/lib/pgsql/archivedir</TD
></TR
><TR
><TD
>レプリケーションスロット</TD
><TD
>有効</TD
><TD
>-</TD
></TR
><TR
><TD
>自動起動</TD
><TD
>自動起動しない</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-PGPOOL-CONFIG"
></A
><P
><B
>表 8-4. Pgpool-IIのバージョンと設定情報</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>項目</TH
><TH
>値</TH
><TH
>説明</TH
></TR
></THEAD
><TBODY
><TR
><TD
>Pgpool-IIバージョン</TD
><TD
>4.1</TD
><TD
>-</TD
></TR
><TR
><TD
ROWSPAN="4"
>ポート番号</TD
><TD
>9999</TD
><TD
>Pgpool-IIが接続を受け付けるポート番号</TD
></TR
><TR
><TD
>9898</TD
><TD
>PCPプロセスが接続を受け付けるポート番号</TD
></TR
><TR
><TD
>9000</TD
><TD
>watchdogが接続を受け付けるポート番号</TD
></TR
><TR
><TD
>9694</TD
><TD
>Watchdogのハートビート信号を受信するUDPポート番号</TD
></TR
><TR
><TD
>設定ファイル</TD
><TD
>/etc/pgpool-II/pgpool.conf</TD
><TD
>Pgpool-IIの設定ファイル</TD
></TR
><TR
><TD
>Pgpool-II起動ユーザ</TD
><TD
>root</TD
><TD
>通常のユーザで<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を起動する場合の設定方法は<A
HREF="tutorial-watchdog-intro.html#TUTORIAL-WATCHDOG-START-STOP"
>項4.1.7</A
>をご参照ください。</TD
></TR
><TR
><TD
>Pgpool-II動作モード</TD
><TD
>ストリーミングレプリケーションモード</TD
><TD
>-</TD
></TR
><TR
><TD
>Watchdog機能</TD
><TD
>有効</TD
><TD
>ハードビート方式</TD
></TR
><TR
><TD
>自動起動</TD
><TD
>自動起動しない</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-INSTALLATION"
>8.3.3. インストール</A
></H2
><P
>   すべてのサーバに<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 11.1と<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1をRPMからインストールします。
  </P
><P
>   <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>のインストールは<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>コミュニティのリポジトリを使います。
  </P
><PRE
CLASS="PROGRAMLISTING"
>   # yum install https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/pgdg-centos11-11-2.noarch.rpm
   # yum install postgresql11 postgresql11-libs postgresql11-devel postgresql11-server
  </PRE
><P
>   <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のインストールは<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>開発コミュニティが提供するYumリポジトリを用いてインストールします。
  </P
><PRE
CLASS="PROGRAMLISTING"
>   # yum install http://www.pgpool.net/yum/rpms/4.1/redhat/rhel-7-x86_64/pgpool-II-release-4.1-1.noarch.rpm
   # yum install pgpool-II-pg11-*
  </PRE
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PRE-SETUP"
>8.3.4. 事前設定</A
></H2
><P
></P
><UL
><LI
><P
>     <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>プライマリサーバのみでストリーミングレプリケーションの設定を行います。設定方法についてはここでは省略します。
     スタンバイサーバの設定は、プライマリが起動した状態で、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のオンラインリカバリ機能を使って行います。
    </P
></LI
></UL
><P
></P
><UL
><LI
><P
>     この設定の例ではアーカイブリカバリを行うように設定します。
    </P
><P
>     まず、すべてのサーバにて<ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
>を格納するディレクトリ<TT
CLASS="FILENAME"
>/var/lib/pgsql/archivedir</TT
>を事前に作成します。この設定例では、Primaryサーバのみで<ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
>アーカイブをローカルで実施します。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [全サーバ]# su - postgres
     [全サーバ]$ mkdir /var/lib/pgsql/archivedir
    </PRE
><P
>     次に<TT
CLASS="LITERAL"
>server1</TT
>にて、設定ファイル<TT
CLASS="FILENAME"
>$PGDATA/postgresql.conf</TT
>を以下のように編集します。
     <TT
CLASS="LITERAL"
>pg_rewind</TT
>を使うために<TT
CLASS="LITERAL"
>wal_log_hints</TT
>を有効にしておきます。
     プライマリが後でスタンバイになる可能性があるので、<TT
CLASS="VARNAME"
>hot_standby = on</TT
>にしておきます。

    </P
><PRE
CLASS="PROGRAMLISTING"
>     listen_addresses = '*'
     archive_mode = on
     archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
     max_wal_senders = 10
     max_replication_slots = 10
     wal_level = replica
     hot_standby = on
     wal_log_hints = on 
    </PRE
></LI
><LI
><P
>     Pgpool-IIのヘルスチェック及びレプリケーションの遅延チェックでPostgreSQLのユーザを設定する必要があります。セキュリティ上の理由で、この設定例ではスーパーユーザを使わないようにします。
     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のレプリケーションの遅延チェックとヘルスチェック用のユーザ<TT
CLASS="LITERAL"
>pgpool</TT
>を作成します。
     また、<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>プライマリサーバ<TT
CLASS="LITERAL"
>server1</TT
>でレプリケーション専用ユーザ<TT
CLASS="LITERAL"
>repl</TT
>を作成します。
     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>4.0からSCRAM認証を利用できるようになりました。この設定例では、<TT
CLASS="LITERAL"
>scram-sha-256</TT
>認証方式を利用します。
     まず、<TT
CLASS="LITERAL"
>password_encryption = 'scram-sha-256'</TT
>に変更してから、ユーザを登録します。
    </P
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-USER"
></A
><P
><B
>表 8-5. ユーザ</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>ユーザ名</TH
><TH
>パスワード</TH
><TH
>備考</TH
></TR
></THEAD
><TBODY
><TR
><TD
>repl</TD
><TD
>repl</TD
><TD
>PostgreSQLのレプリケーション専用ユーザ</TD
></TR
><TR
><TD
>pgpool</TD
><TD
>pgpool</TD
><TD
>Pgpool-IIのレプリケーション遅延チェック、ヘルスチェック専用ユーザ</TD
></TR
><TR
><TD
>postgres</TD
><TD
>postgres</TD
><TD
>オンラインリカバリを実行するユーザ</TD
></TR
></TBODY
></TABLE
></DIV
><PRE
CLASS="PROGRAMLISTING"
>     [server1]# psql -U postgres -p 5432
     postgres=# SET password_encryption = 'scram-sha-256';
     postgres=# CREATE ROLE pgpool WITH LOGIN;
     postgres=# CREATE ROLE repl WITH REPLICATION LOGIN;
     postgres=# \password pgpool
     postgres=# \password repl
     postgres=# \password postgres
    </PRE
><P
>     <A
HREF="sql-show-pool-nodes.html"
>SHOW POOL_NODES</A
>コマンドでレプリケーション状態と同期レプリケーション状態を表示するには、<TT
CLASS="LITERAL"
>pgpool</TT
>ユーザにデフォルトロール<TT
CLASS="LITERAL"
>pg_monitor</TT
>を付与する必要があります (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>4.1以降)。
      以下のコマンドで<TT
CLASS="LITERAL"
>pgpool</TT
>ユーザをそのグループに所属させます。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     GRANT pg_monitor TO pgpool;
    </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>注意: </B
>      <A
HREF="runtime-config-failover.html#GUC-DETACH-FALSE-PRIMARY"
>detach_false_primary</A
>(<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1以降)を利用する予定がある場合、"pgpool" ロールは<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>のスーパーユーザーであるか、<TT
CLASS="LITERAL"
>pg_monitor</TT
>グループに所属する必要があります。
     </P
></BLOCKQUOTE
></DIV
><P
>     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>サーバと<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>バックエンドサーバが同じサブネットワークにあることを想定し、各ユーザが<TT
CLASS="LITERAL"
>scram-sha-256</TT
>認証方式で接続できるように、<TT
CLASS="FILENAME"
>pg_hba.conf</TT
>を編集しておきます。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     host    all             all             samenet                 scram-sha-256
     host    replication     all             samenet                 scram-sha-256
    </PRE
></LI
><LI
><P
>     自動フェイルオーバ、オンラインリカバリ機能を利用するには、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>起動ユーザ(デフォルトでは<TT
CLASS="LITERAL"
>root</TT
>)と<TT
CLASS="LITERAL"
>postgres</TT
>ユーザ間、<TT
CLASS="LITERAL"
>postgres</TT
>ユーザと<TT
CLASS="LITERAL"
>postgres</TT
>ユーザ間が双方向に<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>パスワードなし</I
></SPAN
>で<TT
CLASS="LITERAL"
>SSH</TT
>接続できる状態になっている必要があります。全サーバで以下のコマンドを実行し、<TT
CLASS="LITERAL"
>SSH</TT
>の設定を行います。生成される鍵ファイル名は<TT
CLASS="LITERAL"
>id_rsa_pgpool</TT
>とします。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [全サーバ]# cd ~/.ssh
     [全サーバ]# ssh-keygen -t rsa -f id_rsa_pgpool
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3

     [全サーバ]# su - postgres
     [全サーバ]$ cd ~/.ssh
     [全サーバ]$ ssh-keygen -t rsa -f id_rsa_pgpool
     [全サーバ]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3
    </PRE
><P
>     設定後、<TT
CLASS="LITERAL"
>root</TT
>ユーザ及び<TT
CLASS="LITERAL"
>postgres</TT
>ユーザから<TT
CLASS="COMMAND"
>ssh postgres@serverX -i ~/.ssh/id_rsa_pgpool</TT
>コマンドを実行し、パスワード入力せずログインできることを確認してください。
     必要に応じて<TT
CLASS="FILENAME"
>/etc/ssh/sshd_config</TT
>を編集し、sshdを再起動してください。
    </P
></LI
><LI
><P
>     <TT
CLASS="LITERAL"
>repl</TT
>ユーザのパスワード入力なしで、ストリーミングレプリケーションとオンラインリカバリを行うために、または<TT
CLASS="LITERAL"
>postgres</TT
>ユーザで<SPAN
CLASS="APPLICATION"
>pg_rewind</SPAN
>を実行するために、すべてのサーバにて<TT
CLASS="LITERAL"
>postgres</TT
>ユーザのホームディレクト<TT
CLASS="FILENAME"
>/var/lib/pgsql</TT
> に<TT
CLASS="FILENAME"
>.pgpass</TT
>を作成・配置し、パーミッションを 600 に設定しておきます。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [全サーバ]# su - postgres
     [全サーバ]$ vi /var/lib/pgsql/.pgpass
     (以下を追加)
     server1:5432:replication:repl:&lt;replユーザのパスワード&gt;
     server2:5432:replication:repl:&lt;replユーザのパスワード&gt;
     server3:5432:replication:repl:&lt;replユーザのパスワード&gt;
     server1:5432:postgres:postgres:&lt;postgresユーザのパスワード&gt;
     server2:5432:postgres:postgres:&lt;postgresユーザのパスワード&gt;
     server3:5432:postgres:postgres:&lt;postgresユーザのパスワード&gt;
     [全サーバ]$ chmod 600  /var/lib/pgsql/.pgpass
    </PRE
></LI
><LI
><P
>     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>や<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>に接続する際には、ファイアーウォールによって目的のポートが開けられていなければなりません。<SPAN
CLASS="SYSTEMITEM"
>CentOS/RHEL7</SPAN
>の場合、以下のように設定します。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [全サーバ]# firewall-cmd --permanent --zone=public --add-service=postgresql
     [全サーバ]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/udp
     [全サーバ]# firewall-cmd --reload
    </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG"
>8.3.5. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の設定</A
></H2
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-COMMON"
>8.3.5.1. 共通設定</A
></H3
><P
>    以下の操作は<TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>での共通の設定です。
   </P
><P
>    RPMからインストールした場合、すべての<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の設定ファイルは<TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>にあります。今回はストリーミングレプリケーションモードのテンプレートとして<TT
CLASS="FILENAME"
>pgpool.conf.sample-stream</TT
>サンプルファイルを使用します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# cp /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
   </PRE
><P
>    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>が全てのIPアドレスから接続を受け付けるように、<A
HREF="runtime-config-connection.html#GUC-LISTEN-ADDRESSES"
>listen_addresses</A
>パラメータに<TT
CLASS="LITERAL"
>'*'</TT
>を設定します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    listen_addresses = '*'
   </PRE
><P
>    レプリケーションの遅延チェックユーザ<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
>にpgpoolユーザを設定します。
     この設定例では、<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>は<TT
CLASS="FILENAME"
>pgpool.conf</TT
>に指定せず、<A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>ファイルに作成します。<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0から、<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>が空白の場合、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>は空のパスワードを使用する前にまず<A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>ファイルから<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
>に指定したユーザのパスワードを取得できるか試みます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    sr_check_user = 'pgpool'
    sr_check_password = ''
   </PRE
><P
>    自動フェイルオーバのため、ヘルスチェックを有効にします。<A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PERIOD"
>health_check_period</A
>のデフォルト値が0で、これはヘルスチェックが無効であることを意味します。
     また、ネットワークが不安定な場合には、バックエンドが正常であるにも関わらず、ヘルスチェックに失敗し、フェイルオーバや縮退運転が発生してしまう可能性があります。そのようなヘルスチェックの誤検知を防止するため、ヘルスチェックのリトライ回数を<TT
CLASS="VARNAME"
>health_check_max_retries = 3</TT
> に設定しておきます。
     <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-USER"
>health_check_user</A
>、<A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
>は前述の<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
>、<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>と同様に設定します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    health_check_period = 5
    # Health check period
    # Disabled (0) by default
    health_check_timeout = 30
    # Health check timeout
    # 0 means no timeout
    health_check_user = 'pgpool'
    health_check_password = ''

    health_check_max_retries = 3
   </PRE
><P
>    また、バックエンド情報を前述の<TT
CLASS="LITERAL"
>server1</TT
>、<TT
CLASS="LITERAL"
>server2</TT
>及び<TT
CLASS="LITERAL"
>server3</TT
>の設定に従って設定しておきます。複数バックエンドノードを定義する場合、以下のbackend_*などのパラメータ名の末尾にノードIDを表す数字を付加することで複数のバックエンドを指定することができます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # - Backend Connection Settings -

    backend_hostname0 = 'server1'
    # Host name or IP address to connect to for backend 0
    backend_port0 = 5432
    # Port number for backend 0
    backend_weight0 = 1
    # Weight for backend 0 (only in load balancing mode)
    backend_data_directory0 = '/var/lib/pgsql/11/data'
    # Data directory for backend 0
    backend_flag0 = 'ALLOW_TO_FAILOVER'
    # Controls various backend behavior
    # ALLOW_TO_FAILOVER or DISALLOW_TO_FAILOVER
    backend_hostname1 = 'server2'
    backend_port1 = 5432
    backend_weight1 = 1
    backend_data_directory1 = '/var/lib/pgsql/11/data'
    backend_flag1 = 'ALLOW_TO_FAILOVER'

    backend_hostname2 = 'server3'
    backend_port2 = 5432
    backend_weight2 = 1
    backend_data_directory2 = '/var/lib/pgsql/11/data'
    backend_flag2 = 'ALLOW_TO_FAILOVER'
   </PRE
><P
>    <A
HREF="sql-show-pool-nodes.html"
>SHOW POOL_NODES</A
>コマンドでレプリケーション状態と同期レプリケーション状態を表示するには、<A
HREF="runtime-config-backend-settings.html#GUC-BACKEND-APPLICATION-NAME"
>backend_application_name</A
>パラメータを設定する必要があります。ここではそれぞれのホスト名を設定します。(<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1以降)
      </P><PRE
CLASS="PROGRAMLISTING"
>       ...                                                                          
       backend_application_name0 = 'server1'
       ...  
       backend_application_name1 = 'server2'
       ...  
       backend_application_name2 = 'server3'
      </PRE
><P>
   </P
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-FAILOVER"
>8.3.5.2. フェイルオーバの設定</A
></H3
><P
>    <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>バックエンドノードがダウンした時に実行するスクリプトを<A
HREF="runtime-config-failover.html#GUC-FAILOVER-COMMAND"
>failover_command</A
>に設定します。
     また、<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>サーバが3台の場合、プライマリノードのフェイルオーバ後に新しいプライマリからスレーブをリカバリするために<A
HREF="runtime-config-failover.html#GUC-FOLLOW-MASTER-COMMAND"
>follow_master_command</A
>も設定する必要があります。<A
HREF="runtime-config-failover.html#GUC-FOLLOW-MASTER-COMMAND"
>follow_master_command</A
>はプライマリノードのフェイルオーバ後に実行されます。<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>サーバが2台の場合、<A
HREF="runtime-config-failover.html#GUC-FOLLOW-MASTER-COMMAND"
>follow_master_command</A
>の設定は不要です。
   </P
><P
>    それぞれの実行スクリプトの引数は、それぞれ実行時に<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>によってバックエンドの具体的な情報に置き換えられます。各引数の意味は<A
HREF="runtime-config-failover.html#GUC-FAILOVER-COMMAND"
>failover_command</A
>をご参照ください。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
    follow_master_command = '/etc/pgpool-II/follow_master.sh %d %h %p %D %m %M %H %P %r %R'
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>注意: </B
>     <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>%N</I
></SPAN
>、<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>%S</I
></SPAN
>は<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1で追加された引数です。
     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 または以前のバージョンを利用している場合、これらの引数を指定できないので、ご注意ください。
    </P
></BLOCKQUOTE
></DIV
><P
>    <TT
CLASS="FILENAME"
>/etc/pgpool-II/failover.sh</TT
>及び<TT
CLASS="FILENAME"
>/etc/pgpool-II/follow_master.sh</TT
>を作成し、実行権限を与えておきます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # vi /etc/pgpool-II/failover.sh
    # vi /etc/pgpool-II/follow_master.sh
    # chmod +x /etc/pgpool-II/{failover.sh,follow_master.sh}
   </PRE
><P
></P
><UL
><LI
><P
>      /etc/pgpool-II/failover.sh
     </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run by failover_command.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"
OLD_PRIMARY_NODE_HOST="${11}"
OLD_PRIMARY_NODE_PORT="${12}"

PGHOME=/usr/pgsql-11


logger -i -p local1.info failover.sh: start: failed_node_id=$FAILED_NODE_ID old_primary_node_id=$OLD_PRIMARY_NODE_ID failed_host=$FAILED_NODE_HOST new_master_host=$NEW_MASTER_NODE_HOST

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp &#62; /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info failover.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## If Standby node is down, skip failover.
if [ $FAILED_NODE_ID -ne $OLD_PRIMARY_NODE_ID ]; then
    logger -i -p local1.info failover.sh: Standby node is down. Skipping failover.

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$OLD_PRIMARY_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p $OLD_PRIMARY_NODE_PORT -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error failover.sh: drop replication slot "${FAILED_NODE_HOST}" failed
        exit 1
    fi

    exit 0
fi

## Promote Standby node.
logger -i -p local1.info failover.sh: Primary node is down, promote standby node ${NEW_MASTER_NODE_HOST}.

ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote

if [ $? -ne 0 ]; then
    logger -i -p local1.error failover.sh: new_master_host=$NEW_MASTER_NODE_HOST promote failed
    exit 1
fi

logger -i -p local1.info failover.sh: end: new_master_node_id=$NEW_MASTER_NODE_ID started as the primary node
exit 0
     </PRE
></LI
></UL
><P
></P
><UL
><LI
><P
>      /etc/pgpool-II/follow_master.sh
     </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run after failover_command to synchronize the Standby with the new Primary.
# First try pg_rewind. If pg_rewind failed, use pg_basebackup.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
OLD_MASTER_NODE_ID="$6"
NEW_MASTER_NODE_HOST="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPLUSER=repl
PCP_USER=pgpool
PGPOOL_PATH=/usr/bin
PCP_PORT=9898

logger -i -p local1.info follow_master.sh: start: Standby node ${FAILED_NODE_ID}

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp &#62; /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info follow_master.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`

if [ $PGVERSION -ge 12 ]; then
RECOVERYCONF=${FAILED_NODE_PGDATA}/myrecovery.conf
else
RECOVERYCONF=${FAILED_NODE_PGDATA}/recovery.conf
fi

## Check the status of Standby
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -w -D ${FAILED_NODE_PGDATA} status


## If Standby is running, synchronize it with the new Primary.
if [ $? -eq 0 ]; then

    logger -i -p local1.info follow_master.sh: pg_rewind for $FAILED_NODE_ID

    # Create replication slot "${FAILED_NODE_HOST}"
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_create_physical_replication_slot('${FAILED_NODE_HOST}');\"
    "

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

        set -o errexit

        ${PGHOME}/bin/pg_ctl -w -m f -D ${FAILED_NODE_PGDATA} stop

        cat &gt; ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

        if [ ${PGVERSION} -ge 12 ]; then
            touch ${FAILED_NODE_PGDATA}/standby.signal
        else
            echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
        fi

        ${PGHOME}/bin/pg_rewind -D ${FAILED_NODE_PGDATA} --source-server=\"user=postgres host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT}\"

    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error follow_master.sh: end: pg_rewind failed. Try pg_basebackup.

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
             
            set -o errexit

            # Execute pg_basebackup
            rm -rf ${FAILED_NODE_PGDATA}
            rm -rf ${ARCHIVEDIR}/*
            ${PGHOME}/bin/pg_basebackup -h ${NEW_MASTER_NODE_HOST} -U $REPLUSER -p ${NEW_MASTER_NODE_PORT} -D ${FAILED_NODE_PGDATA} -X stream

            if [ ${PGVERSION} -ge 12 ]; then
                sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
                       -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${FAILED_NODE_PGDATA}/postgresql.conf
            fi
     
            cat &#62; ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

            if [ ${PGVERSION} -ge 12 ]; then
                    touch ${FAILED_NODE_PGDATA}/standby.signal
            else
                    echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
            fi
        "

        if [ $? -ne 0 ]; then
            # drop replication slot
            ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
                ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
            "

            logger -i -p local1.error follow_master.sh: end: pg_basebackup failed
            exit 1
        fi
    fi

    # start Standby node on ${FAILED_NODE_HOST}
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool $PGHOME/bin/pg_ctl -l /dev/null -w -D ${FAILED_NODE_PGDATA} start

    # If start Standby successfully, attach this node
    if [ $? -eq 0 ]; then

        # Run pcp_attact_node to attach Standby node to Pgpool-II.
        ${PGPOOL_PATH}/pcp_attach_node -w -h localhost -U $PCP_USER -p ${PCP_PORT} -n ${FAILED_NODE_ID}

        if [ $? -ne 0 ]; then
                logger -i -p local1.error follow_master.sh: end: pcp_attach_node failed
                exit 1
        fi

    # If start Standby failed, drop replication slot "${FAILED_NODE_HOST}"
    else

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool \
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c "SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')"

        logger -i -p local1.error follow_master.sh: end: follow master command failed
        exit 1
    fi

else
    logger -i -p local1.info follow_master.sh: failed_nod_id=${FAILED_NODE_ID} is not running. skipping follow master command
    exit 0
fi

logger -i -p local1.info follow_master.sh: end: follow master command complete
exit 0
     </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-ONLINE-RECOVERY"
>8.3.5.3. オンラインリカバリの設定</A
></H3
><P
>    続いて、オンラインリカバリを行うための<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>のユーザ名およびオンラインリカバリ時に呼び出されるコマンド<TT
CLASS="COMMAND"
>recovery_1st_stage</TT
>を設定します。
    オンラインリカバリで実行される<CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
>関数は<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>のスーパーユーザ権限が必要なため、<TT
CLASS="VARNAME"
>recovery_user</TT
>に<SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>スーパーユーザ</I
></SPAN
>を指定しなければなりません。ここでは、postrgesユーザを指定します。
    オンラインリカバリ用のスクリプト<TT
CLASS="FILENAME"
>recovery_1st_stage</TT
>、<TT
CLASS="FILENAME"
>pgpool_remote_start</TT
>をプライマリサーバ(server1)のデータベースクラスタ配下に配置し、実行権限を与えておきます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    recovery_user = 'postgres'
    # Online recovery user
    recovery_password = ''
    # Online recovery password

    recovery_1st_stage_command = 'recovery_1st_stage'
   </PRE
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# su - postgres
    [server1]$ vi /var/lib/pgsql/11/data/recovery_1st_stage
    [server1]$ vi /var/lib/pgsql/11/data/pgpool_remote_start
    [server1]$ chmod +x /var/lib/pgsql/11/data/{recovery_1st_stage,pgpool_remote_start}
   </PRE
><P
></P
><UL
><LI
><P
>      /var/lib/pgsql/11/data/recovery_1st_stage
     </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is executed by "recovery_1st_stage" to recovery a Standby node.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

PRIMARY_NODE_PGDATA="$1"
DEST_NODE_HOST="$2"
DEST_NODE_PGDATA="$3"
PRIMARY_NODE_PORT="$4"
DEST_NODE_ID="$5"
DEST_NODE_PORT="$6"

PRIMARY_NODE_HOST=$(hostname)
PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPLUSER=repl

logger -i -p local1.info recovery_1st_stage: start: pg_basebackup for Standby node $DEST_NODE_ID

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp &#62; /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info recovery_1st_stage: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`
if [ $PGVERSION -ge 12 ]; then
    RECOVERYCONF=${DEST_NODE_PGDATA}/myrecovery.conf
else
    RECOVERYCONF=${DEST_NODE_PGDATA}/recovery.conf
fi

## Create replication slot "${DEST_NODE_HOST}"
${PGHOME}/bin/psql -p ${PRIMARY_NODE_PORT} &lt;&lt; EOQ
SELECT pg_create_physical_replication_slot('${DEST_NODE_HOST}');
EOQ

## Execute pg_basebackup to recovery Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "

    set -o errexit

    rm -rf $DEST_NODE_PGDATA
    rm -rf $ARCHIVEDIR/*

    ${PGHOME}/bin/pg_basebackup -h $PRIMARY_NODE_HOST -U $REPLUSER -p $PRIMARY_NODE_PORT -D $DEST_NODE_PGDATA -X stream

    if [ ${PGVERSION} -ge 12 ]; then
        sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
               -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${DEST_NODE_PGDATA}/postgresql.conf
    fi

    cat &gt; ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${PRIMARY_NODE_HOST} port=${PRIMARY_NODE_PORT} user=${REPLUSER} application_name=${DEST_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${PRIMARY_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${DEST_NODE_HOST}'
EOT

    if [ ${PGVERSION} -ge 12 ]; then
            touch ${DEST_NODE_PGDATA}/standby.signal
    else
            echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
    fi

    sed -i \"s/#*port = .*/port = ${DEST_NODE_PORT}/\" ${DEST_NODE_PGDATA}/postgresql.conf
"

if [ $? -ne 0 ]; then

    ${PGHOME}/bin/psql -p ${PRIMARY_NODE_PORT} &lt;&lt; EOQ
SELECT pg_drop_replication_slot('${DEST_NODE_HOST}');
EOQ

    logger -i -p local1.error recovery_1st_stage: end: pg_basebackup failed. online recovery failed
    exit 1
fi

logger -i -p local1.info recovery_1st_stage: end: recovery_1st_stage complete
exit 0
     </PRE
></LI
><LI
><P
>      /var/lib/pgsql/11/data/pgpool_remote_start
     </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run after recovery_1st_stage to start Standby node.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

PGHOME=/usr/pgsql-11
DEST_NODE_HOST="$1"
DEST_NODE_PGDATA="$2"


logger -i -p local1.info pgpool_remote_start: start: remote start Standby node $DEST_NODE_HOST

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp &#62; /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info pgpool_remote_start: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Start Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
    $PGHOME/bin/pg_ctl -l /dev/null -w -D $DEST_NODE_PGDATA start
"

if [ $? -ne 0 ]; then
    logger -i -p local1.error pgpool_remote_start: $DEST_NODE_HOST PostgreSQL start failed.
    exit 1
fi

logger -i -p local1.info pgpool_remote_start: end: $DEST_NODE_HOST PostgreSQL started successfully.
exit 0
     </PRE
></LI
></UL
><P
>    また、オンラインリカバリ機能を使用するには、<CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
>、<CODE
CLASS="FUNCTION"
>pgpool_remote_start</CODE
>、<CODE
CLASS="FUNCTION"
>pgpool_switch_xlog</CODE
>という関数が必要になるので、<TT
CLASS="LITERAL"
>server1</TT
>のtemplate1に<CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
>をインストールしておきます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# su - postgres
    [server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-AUTH"
>8.3.5.4. クライアント認証の設定</A
></H3
><P
>    <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>事前設定</A
>の章で、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>と<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>の間に認証方式を<ACRONYM
CLASS="ACRONYM"
>scram-sha-256</ACRONYM
>に設定しました。この設定例では、クライアントと<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の間でも<ACRONYM
CLASS="ACRONYM"
>scram-sha-256</ACRONYM
>認証方式を利用し接続するように設定します。
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のクライアント認証の設定ファイルは<TT
CLASS="FILENAME"
>pool_hba.conf</TT
>と呼ばれ、RPMパッケージからインストールする場合、デフォルトでは<TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>配下にインストールされます。
    デフォルトでは<TT
CLASS="FILENAME"
>pool_hba.conf</TT
>による認証は無効になっているので、<TT
CLASS="FILENAME"
>pgpool.conf</TT
>では以下の設定をonに変更します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    enable_pool_hba = on
   </PRE
><P
>    <TT
CLASS="FILENAME"
>pool_hba.conf</TT
>のフォーマットは<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>の<TT
CLASS="FILENAME"
>pg_hba.conf</TT
>とほとんど同じです。<TT
CLASS="LITERAL"
>pgpool</TT
>と<TT
CLASS="LITERAL"
>postgres</TT
>ユーザを<ACRONYM
CLASS="ACRONYM"
>scram-sha-256</ACRONYM
>認証に設定します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    host    all         pgpool           0.0.0.0/0          scram-sha-256
    host    all         postgres         0.0.0.0/0          scram-sha-256
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>注意: </B
>     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0の場合、<TT
CLASS="FILENAME"
>pgpool.conf</TT
>ファイル内の<A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
>、
      <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>、<A
HREF="runtime-watchdog-config.html#GUC-WD-LIFECHECK-PASSWORD"
>wd_lifecheck_password</A
>、
	<A
HREF="runtime-online-recovery.html#GUC-RECOVERY-PASSWORD"
>recovery_password</A
>にはAES256暗号化形式、平文形式しか指定できないので、
	 ご注意ください。
    </P
></BLOCKQUOTE
></DIV
><P
>    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のクライアント認証で用いるデフォルトのパスワードファイル名はpool_passwdです。
    <TT
CLASS="LITERAL"
>scram-sha-256</TT
>認証を利用する場合、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>はそれらのパスワードを復号化するために復号鍵が必要となります。全サーバで復号鍵ファイルをrootユーザのホームディレクトリ配下に作成します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# echo '任意の文字列' &#62; ~/.pgpoolkey 
    [全サーバ]# chmod 600 ~/.pgpoolkey
   </PRE
><P
>    「pg_enc -m -k /path/to/.pgpoolkey -u ユーザ名 -p」 コマンドを実行すると、ユーザ名と<TT
CLASS="LITERAL"
>AES256</TT
>で暗号化したパスワードのエントリが<A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>に登録されます。 
     <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
> がまだ存在しなければ、<TT
CLASS="FILENAME"
>pgpool.conf</TT
>と同じディレクトリ内に作成されます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# pg_enc -m -k /root/.pgpoolkey -u pgpool -p
    db password: [pgpoolユーザのパスワード]
    [全サーバ]# pg_enc -m -k /root/.pgpoolkey -u postgres -p
    db password: [postgresユーザのパスワード]

    # cat /etc/pgpool-II/pool_passwd 
    pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
    postgres:AESHs/pWL5rtXy2IwuzroHfqg==
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-WATCHDOG"
>8.3.5.5. Watchdogの設定</A
></H3
><P
>    デフォルトでは<TT
CLASS="LITERAL"
>watchdog</TT
>機能が無効のため、<TT
CLASS="LITERAL"
>server1</TT
>、<TT
CLASS="LITERAL"
>server2</TT
>及び<TT
CLASS="LITERAL"
>server3</TT
>で<TT
CLASS="LITERAL"
>watchdog</TT
>を有効にします。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    use_watchdog = on
   </PRE
><P
>    アクティブ機が立ち上げる仮想IPをdelegate_IPに指定します。仮想 IP はまだ使われていないIPアドレスを指定してください。<TT
CLASS="LITERAL"
>server1</TT
>、<TT
CLASS="LITERAL"
>server2</TT
>及び<TT
CLASS="LITERAL"
>server3</TT
>の共通の設定です。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    delegate_IP = '192.168.137.150'
   </PRE
><P
>    仮想IPの起動/停止、ARPリクエストの送信を行う設定パラメータ<A
HREF="runtime-watchdog-config.html#GUC-IF-UP-CMD"
>if_up_cmd</A
>、<A
HREF="runtime-watchdog-config.html#GUC-IF-DOWN-CMD"
>if_down_cmd</A
>、<A
HREF="runtime-watchdog-config.html#GUC-ARPING-CMD"
>arping_cmd</A
>に、ネットワーク環境に合わせてネットワークインターフェース名を設定します。
       今回の例で使ったネットワークインターフェースは「enp0s8」となっています。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    if_up_cmd = 'ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
    # startup delegate IP command
    if_down_cmd = 'ip addr del $_IP_$/24 dev enp0s8'
    # shutdown delegate IP command
    arping_cmd = 'arping -U $_IP_$ -w 1 -I enp0s8'
    # arping command
   </PRE
><P
>    ipコマンドやarpingコマンドのパスがデフォルトのパスと異なる場合、環境に合わせて<A
HREF="runtime-watchdog-config.html#GUC-IF-CMD-PATH"
>if_cmd_path</A
>や<A
HREF="runtime-watchdog-config.html#GUC-ARPING-PATH"
>arping_path</A
>を設定しておいてください。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    if_cmd_path = '/sbin'
    # path to the directory where if_up/down_cmd exists
    arping_path = '/usr/sbin'
    # arping command path
   </PRE
><P
>    各watchdog が稼働するサーバ情報を設定しておきます。
   </P
><P
></P
><UL
><LI
><P
>      <TT
CLASS="LITERAL"
>server1</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      wd_hostname = 'server1'
      wd_port = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server2</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      wd_hostname = 'server2'
      wd_port = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server3</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      wd_hostname = 'server3'
      wd_port = 9000
     </PRE
></LI
></UL
><P
>    各監視対象の<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>サーバ情報を設定しておきます。
   </P
><P
></P
><UL
><LI
><P
>      <TT
CLASS="LITERAL"
>server1</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server2'
      # Host name or IP address to connect to for other pgpool 0
      # (change requires restart)
      other_pgpool_port0 = 9999
      # Port number for other pgpool 0
      # (change requires restart)
      other_wd_port0 = 9000
      # Port number for other watchdog 0
      # (change requires restart)
      other_pgpool_hostname1 = 'server3'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server2</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server1'
      # Host name or IP address to connect to for other pgpool 0
      # (change requires restart)
      other_pgpool_port0 = 9999
      # Port number for other pgpool 0
      # (change requires restart)
      other_wd_port0 = 9000
      # Port number for other watchdog 0
      # (change requires restart)
      other_pgpool_hostname1 = 'server3'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server3</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server1'
      # Host name or IP address to connect to for other pgpool 0
      # (change requires restart)
      other_pgpool_port0 = 9999
      # Port number for other pgpool 0
      # (change requires restart)
      other_wd_port0 = 9000
      # Port number for other watchdog 0
      # (change requires restart)
      other_pgpool_hostname1 = 'server2'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </PRE
></LI
></UL
><P
>    ハートビート信号の送信先のホスト名とポート番号を指定します。
   </P
><P
></P
><UL
><LI
><P
>      <TT
CLASS="LITERAL"
>server1</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      heartbeat_destination0 = 'server2'
      # Host name or IP address of destination 0
      # for sending heartbeat signal.
      # (change requires restart)
      heartbeat_destination_port0 = 9694
      # Port number of destination 0 for sending
      # heartbeat signal. Usually this is the
      # same as wd_heartbeat_port.
      # (change requires restart)
      heartbeat_device0 = ''
      # Name of NIC device (such like 'eth0')
      # used for sending/receiving heartbeat
      # signal to/from destination 0.
      # This works only when this is not empty
      # and pgpool has root privilege.
      # (change requires restart)

      heartbeat_destination1 = 'server3'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''

     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server2</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      heartbeat_destination0 = 'server1'
      # Host name or IP address of destination 0
      # for sending heartbeat signal.
      # (change requires restart)
      heartbeat_destination_port0 = 9694
      # Port number of destination 0 for sending
      # heartbeat signal. Usually this is the
      # same as wd_heartbeat_port.
      # (change requires restart)
      heartbeat_device0 = ''
      # Name of NIC device (such like 'eth0')
      # used for sending/receiving heartbeat
      # signal to/from destination 0.
      # This works only when this is not empty
      # and pgpool has root privilege.
      # (change requires restart)

      heartbeat_destination1 = 'server3'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''

     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server3</TT
>の場合
     </P
><PRE
CLASS="PROGRAMLISTING"
>      heartbeat_destination0 = 'server1'
      # Host name or IP address of destination 0
      # for sending heartbeat signal.
      # (change requires restart)
      heartbeat_destination_port0 = 9694
      # Port number of destination 0 for sending
      # heartbeat signal. Usually this is the
      # same as wd_heartbeat_port.
      # (change requires restart)
      heartbeat_device0 = ''
      # Name of NIC device (such like 'eth0')
      # used for sending/receiving heartbeat
      # signal to/from destination 0.
      # This works only when this is not empty
      # and pgpool has root privilege.
      # (change requires restart)

      heartbeat_destination1 = 'server2'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''
     </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-SYSCONFIG"
>8.3.5.6. /etc/sysconfig/pgpoolの設定</A
></H3
><P
>    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を起動時に<TT
CLASS="FILENAME"
>pgpool_status</TT
>ファイルを無視させたい場合、<TT
CLASS="FILENAME"
>/etc/sysconfig/pgpool</TT
>の起動オプションOPTSに「-D」を追加します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# vi /etc/sysconfig/pgpool 
    (...省略...)
    OPTS=" -D -n"
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-LOG"
>8.3.5.7. ログの設定</A
></H3
><P
>    この例では、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のログ出力は<TT
CLASS="LITERAL"
>syslog</TT
>を利用するように設定します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    log_destination = 'syslog'
    # Where to log
    # Valid values are combinations of stderr,
    # and syslog. Default to stderr.

    syslog_facility = 'LOCAL1'
    # Syslog local facility. Default to LOCAL0
   </PRE
><P
>    全サーバではログファイルを作成します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# mkdir /var/log/pgpool-II
    [全サーバ]# touch /var/log/pgpool-II/pgpool.log
   </PRE
><P
>    次に<TT
CLASS="LITERAL"
>syslog</TT
>の設定ファイルを以下のように編集します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# vi /etc/rsyslog.conf
    ...(省略)...
    *.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
    LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
   </PRE
><P
>    また、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>に関して<TT
CLASS="FILENAME"
>/var/log/messages</TT
>と同様のログローテーションを行うように、logrotateの設定を以下のように行います。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# vi /etc/logrotate.d/syslog
    ...(省略)...
    /var/log/messages
    /var/log/pgpool-II/pgpool.log
    /var/log/secure
   </PRE
><P
>    設定が終わったら、rsyslogサービスを再起動します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# systemctl restart rsyslog
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PCP"
>8.3.5.8. PCPコマンドの設定</A
></H3
><P
>    <TT
CLASS="LITERAL"
>PCP</TT
>コマンドを使用するにはユーザ認証が必要になるので、ユーザ名と<TT
CLASS="LITERAL"
>md5</TT
>ハッシュに変換されたパスワードを<TT
CLASS="FILENAME"
>pcp.conf</TT
>ファイルに設定します。
    ここではユーザ名に<TT
CLASS="LITERAL"
>pgpool</TT
>を使用し、以下のコマンドを実行することで、&lt;ユーザ名:ハッシュ化されたパスワード&gt;が<TT
CLASS="FILENAME"
>/etc/pgpool-II/pcp.conf</TT
>に追加されます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# echo 'pgpool:'`pg_md5 PCPコマンドパスワード` &gt;&gt; /etc/pgpool-II/pcp.conf
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PCPPASS"
>8.3.5.9. .pcppassの設定</A
></H3
><P
>    前述の<TT
CLASS="LITERAL"
>follow_master_command</TT
>のスクリプトでパスワード入力なしで<TT
CLASS="LITERAL"
>PCP</TT
>コマンドを実行できるように、すべてのサーバで<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の起動ユーザのホームディレクトリに<TT
CLASS="FILENAME"
>.pcppass</TT
>を作成します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [全サーバ]# echo 'localhost:9898:pgpool:pgpool' &#62; ~/.pcppass
    [全サーバ]# chmod 600 ~/.pcppass
   </PRE
><P
>    ここで、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の設定は完了です。
   </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-START-STOP"
>8.3.6. システムの起動と停止</A
></H2
><P
>   <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の設定が完了したら、次に<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を起動します。<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を起動する前に、バックエンドの<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>をあらかじめ起動する必要があります。また、<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>を停止する場合、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を先に停止する必要があります。
  </P
><P
></P
><UL
><LI
><P
>     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の起動
    </P
><P
>     前述の<A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>事前設定</A
>の章で<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の自動起動が設定済なので、ここでシステム全体を再起動するか、以下のコマンドを実行してください。
    </P
><PRE
CLASS="PROGRAMLISTING"
>     # systemctl start pgpool.service
    </PRE
></LI
><LI
><P
>     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の停止
    </P
><PRE
CLASS="PROGRAMLISTING"
>     # systemctl stop pgpool.service
    </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-TRY"
>8.3.7. 動作確認</A
></H2
><P
>   これから、動作確認を行います。まず、<TT
CLASS="LITERAL"
>server1</TT
>、<TT
CLASS="LITERAL"
>server2</TT
>、<TT
CLASS="LITERAL"
>server3</TT
>で以下のコマンドで<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を起動します。
  </P
><PRE
CLASS="PROGRAMLISTING"
>   # systemctl start pgpool.service
  </PRE
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-STANDBY"
>8.3.7.1. PostgreSQL スタンバイサーバを構築</A
></H3
><P
>    まず、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のオンラインリカバリ機能を利用し、スタンバイサーバを構築します。<TT
CLASS="COMMAND"
>pcp_recovery_node</TT
>コマンドで実行される<TT
CLASS="VARNAME"
>recovery_1st_stage_command</TT
>パラメータに指定した<TT
CLASS="FILENAME"
>recovery_1st_stage</TT
>と<TT
CLASS="FILENAME"
>pgpool_remote_start</TT
>スプリクトが実行されるので、この 2つのスクリプトが現在稼働中のプライマリサーバ<TT
CLASS="LITERAL"
>server1</TT
>のデータベースクラスタの下に存在することを確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 1
    Password: 
    pcp_recovery_node -- Command Successful

    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 2
    Password: 
    pcp_recovery_node -- Command Successful
   </PRE
><P
>    <TT
CLASS="LITERAL"
>server2</TT
>と<TT
CLASS="LITERAL"
>server3</TT
>の<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>がスタンバイとして起動されていることを確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
    1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
    (3 rows)
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-WATCHDOG"
>8.3.7.2. watchdogアクティブ/スタンバイの切り替え</A
></H3
><P
>    <TT
CLASS="COMMAND"
>pcp_watchdog_info</TT
>で<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>の<SPAN
CLASS="APPLICATION"
>watchdog</SPAN
>の情報を確認します。最初に起動した<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>が「MASTER」になります。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # pcp_watchdog_info -h 192.168.137.150 -p 9898 -U pgpool
    Password: 
    3 YES server1:9999 Linux server1 server1

    server1:9999 Linux server1 server1 9999 9000 4 MASTER  #最初に起動されたサーバがMASTERになる
    server2:9999 Linux server2 server2 9999 9000 7 STANDBY #スタンバイとして稼働
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY #スタンバイとして稼働
   </PRE
><P
>    アクティブである<TT
CLASS="LITERAL"
>server1</TT
>の<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を停止し、<TT
CLASS="LITERAL"
>server2</TT
>または<TT
CLASS="LITERAL"
>server3</TT
>がスタンバイからアクティブに昇格することを確認します。<TT
CLASS="LITERAL"
>server1</TT
>を停止する方法は<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を停止する、またはマシンをシャットダウンします。ここでは、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を停止します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# systemctl stop pgpool.service

    # pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
    Password: 
    3 YES server2:9999 Linux server2 server2

    server2:9999 Linux server2 server2 9999 9000 4 MASTER     #server2がアクティブに昇格
    server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN  #server1が停止された
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY    #スタンバイとして稼働
   </PRE
><P
>    先ほど停止した<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>を再起動し、スタンバイとして起動したことを確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# systemctl start pgpool.service

    [server1]# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
    Password: 
    3 YES server2:9999 Linux server2 server2

    server2:9999 Linux server2 server2 9999 9000 4 MASTER
    server1:9999 Linux server1 server1 9999 9000 7 STANDBY
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-FAILOVER"
>8.3.7.3. 自動フェイルオーバ</A
></H3
><P
>    <TT
CLASS="COMMAND"
>psql</TT
>で仮想IPに接続し、バックエンドの情報を確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
    1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
    (3 rows)
   </PRE
><P
>    次にプライマリである<TT
CLASS="LITERAL"
>server1</TT
>の<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>を停止し、フェイルオーバするかどうか確認してみます。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]$ pg_ctl -D /var/lib/pgsql/11/data -m immediate stop
   </PRE
><P
>    <TT
CLASS="LITERAL"
>ノード1</TT
>を停止後、フェイルオーバが発生し、<TT
CLASS="LITERAL"
>server2</TT
>がプライマリに昇格したことを確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
    1       | server2  | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 |                   |                        | 2019-08-06 11:36:03
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:36:15
    (3 rows)
   </PRE
><P
>    <TT
CLASS="LITERAL"
>server3</TT
>が新しいプライマリ<TT
CLASS="LITERAL"
>server2</TT
>のスタンバイとして起動されています。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    t

    [server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    f

    [server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select * from pg_stat_replication" -x
    -[ RECORD 1 ]----+------------------------------
    pid              | 11059
    usesysid         | 16392
    usename          | repl
    application_name | server3
    client_addr      | 192.168.137.103
    client_hostname  | 
    client_port      | 48694
    backend_start    | 2019-08-06 11:36:07.479161+09
    backend_xmin     | 
    state            | streaming
    sent_lsn         | 0/75000148
    write_lsn        | 0/75000148
    flush_lsn        | 0/75000148
    replay_lsn       | 0/75000148
    write_lag        | 
    flush_lag        | 
    replay_lag       | 
    sync_priority    | 0
    sync_state       | async
    reply_time       | 2019-08-06 11:42:59.823961+09
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-ONLINE-RECOVERY"
>8.3.7.4. オンラインリカバリ</A
></H3
><P
>    次に、<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>のオンラインリカバリ機能を利用し、先ほど停止した旧プライマリサーバをスタンバイとして復旧させます。<TT
CLASS="COMMAND"
>pcp_recovery_node</TT
>コマンドで実行される<TT
CLASS="VARNAME"
>recovery_1st_stage_command</TT
>パラメータに指定した<TT
CLASS="FILENAME"
>recovery_1st_stage</TT
>と<TT
CLASS="FILENAME"
>pgpool_remote_start</TT
>スプリクトが現在稼働中のプライマリサーバ<TT
CLASS="LITERAL"
>server2</TT
>のデータベースクラスタの下に存在することを確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 0
    Password: 
    pcp_recovery_node -- Command Successful
   </PRE
><P
>    <TT
CLASS="LITERAL"
>ノード1</TT
>がスタンバイとして起動されたことを確認します。
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:48:05
    1       | server2  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:36:15
    (3 rows)
   </PRE
><P
>    以上で、動作確認が完了です。
   </P
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-watchdog.html"
ACCESSKEY="P"
>前のページ</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>ホーム</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="example-aws.html"
ACCESSKEY="N"
>次のページ</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Watchdogの設定例</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>上に戻る</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>AWS設定の例</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>