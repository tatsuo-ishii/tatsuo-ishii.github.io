<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Pgpool-II + Watchdog Setup Example</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:pgsql-docs@postgresql.org"><LINK
REL="HOME"
TITLE="pgpool-II 4.2devel Documentation"
HREF="index.html"><LINK
REL="UP"
TITLE="Configuration Examples"
HREF="example-configs.html"><LINK
REL="PREVIOUS"
TITLE="Watchdog Configuration Example"
HREF="example-watchdog.html"><LINK
REL="NEXT"
TITLE="AWS Configuration Example"
HREF="example-aws.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stylesheet.css"><META
HTTP-EQUIV="Content-Type"
CONTENT="text/html; charset=ISO-8859-1"><META
NAME="creation"
CONTENT="2020-09-22T02:14:39"></HEAD
><BODY
CLASS="SECT1"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="4"
ALIGN="center"
VALIGN="bottom"
><A
HREF="index.html"
>pgpool-II 4.2devel Documentation</A
></TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
TITLE="Watchdog Configuration Example"
HREF="example-watchdog.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="60%"
ALIGN="center"
VALIGN="bottom"
>Chapter 8. Configuration Examples</TD
><TD
WIDTH="20%"
ALIGN="right"
VALIGN="top"
><A
TITLE="AWS Configuration Example"
HREF="example-aws.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="EXAMPLE-CLUSTER"
>8.3. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> + Watchdog Setup Example</A
></H1
><P
>  This section shows an example of streaming replication configuration using
  <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. In this example, we use 3
  <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers to manage <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>
  servers to create a robust cluster system and avoid the single point of failure or split brain.
 </P
><P
>  <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 11 is used in this configuration example,
  all scripts have also been tested with <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 12.
 </P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-REQUIREMENT"
>8.3.1. Requirements</A
></H2
><P
>   We assume that all the Pgpool-II servers and the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers are in the same subnet.
  </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-STRUCTURE"
>8.3.2. Cluster System Configuration</A
></H2
><P
>   We use 3 servers with CentOS 7.4. Let these servers be <TT
CLASS="LITERAL"
>server1</TT
>,
   <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>. 
   We install <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> and <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> on each server.
  </P
><P
>   <DIV
CLASS="FIGURE"
><A
NAME="AEN6549"
></A
><P
><B
>Figure 8-1. Cluster System Configuration</B
></P
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG
SRC="cluster_40.gif"></P
></DIV
></DIV
>
  </P
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>    The roles of <TT
CLASS="LITERAL"
>Active</TT
>, <TT
CLASS="LITERAL"
>Standy</TT
>, <TT
CLASS="LITERAL"
>Primary</TT
>,
    <TT
CLASS="LITERAL"
>Standby</TT
> are not fixed and may be changed by further operations.
   </P
></BLOCKQUOTE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-IP"
></A
><P
><B
>Table 8-2. Hostname and IP address</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Hostname</TH
><TH
>IP Address</TH
><TH
>Virtual IP</TH
></TR
></THEAD
><TBODY
><TR
><TD
>server1</TD
><TD
>192.168.137.101</TD
><TD
ROWSPAN="3"
>192.168.137.150</TD
></TR
><TR
><TD
>server2</TD
><TD
>192.168.137.102</TD
></TR
><TR
><TD
>server3</TD
><TD
>192.168.137.103</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-POSTGRESQL-CONFIG"
></A
><P
><B
>Table 8-3. PostgreSQL version and Configuration</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Item</TH
><TH
>Value</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>PostgreSQL Version</TD
><TD
>11.1</TD
><TD
>-</TD
></TR
><TR
><TD
>port</TD
><TD
>5432</TD
><TD
>-</TD
></TR
><TR
><TD
>$PGDATA</TD
><TD
>/var/lib/pgsql/11/data</TD
><TD
>-</TD
></TR
><TR
><TD
>Archive mode</TD
><TD
>on</TD
><TD
>/var/lib/pgsql/archivedir</TD
></TR
><TR
><TD
>Replication Slots</TD
><TD
>Enable</TD
><TD
>-</TD
></TR
><TR
><TD
>Start automatically</TD
><TD
>Disable</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-PGPOOL-CONFIG"
></A
><P
><B
>Table 8-4. Pgpool-II version and Configuration</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Item</TH
><TH
>Value</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>Pgpool-II Version</TD
><TD
>4.1.0</TD
><TD
>-</TD
></TR
><TR
><TD
ROWSPAN="4"
>port</TD
><TD
>9999</TD
><TD
>Pgpool-II accepts connections</TD
></TR
><TR
><TD
>9898</TD
><TD
>PCP process accepts connections</TD
></TR
><TR
><TD
>9000</TD
><TD
>watchdog accepts connections</TD
></TR
><TR
><TD
>9694</TD
><TD
>UDP port for receiving Watchdog's heartbeat signal</TD
></TR
><TR
><TD
>Config file</TD
><TD
>/etc/pgpool-II/pgpool.conf</TD
><TD
>Pgpool-II config file</TD
></TR
><TR
><TD
>Pgpool-II start user</TD
><TD
>postgres (Pgpool-II 4.1 or later)</TD
><TD
>Pgpool-II 4.0 or before, the default startup user is root</TD
></TR
><TR
><TD
>Running mode</TD
><TD
>streaming replication mode</TD
><TD
>-</TD
></TR
><TR
><TD
>Watchdog</TD
><TD
>on</TD
><TD
>Life check method: heartbeat</TD
></TR
><TR
><TD
>Start automatically</TD
><TD
>Disable</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-INSTALLATION"
>8.3.3. Installation</A
></H2
><P
>   In this example, we install <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 11.1 by using RPM packages.
  </P
><P
>   Install <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> by using <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> YUM repository.
  </P
><PRE
CLASS="PROGRAMLISTING"
>   # yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
   # yum install postgresql11-server
  </PRE
><P
>   Install <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> by using Pgpool-II YUM repository.
  </P
><PRE
CLASS="PROGRAMLISTING"
>   # yum install http://www.pgpool.net/yum/rpms/4.1/redhat/rhel-7-x86_64/pgpool-II-release-4.1-2.noarch.rpm
   # yum install pgpool-II-pg11-*
  </PRE
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PRE-SETUP"
>8.3.4. Before Starting</A
></H2
><P
>   Before you start the configuration process, please check the following prerequisites.
  </P
><P
></P
><UL
><LI
><P
>     Set up <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> streaming replication on the primary server.
     In this example, we use WAL archiving.
    </P
><P
>     First, we create the directory <TT
CLASS="FILENAME"
>/var/lib/pgsql/archivedir</TT
> to store
     <ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
> segments on all servers. In this example, only Primary node archives
     <ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
> locally.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [all servers]# su - postgres
     [all servers]$ mkdir /var/lib/pgsql/archivedir
    </PRE
><P
>     Then we edit the configuration file <TT
CLASS="FILENAME"
>$PGDATA/postgresql.conf</TT
>
     on <TT
CLASS="LITERAL"
>server1</TT
> (primary) as follows. Enable <TT
CLASS="LITERAL"
>wal_log_hints</TT
>
     to use <TT
CLASS="LITERAL"
>pg_rewind</TT
>. 
     Since the Primary may become a Standby later, we set <TT
CLASS="VARNAME"
>hot_standby = on</TT
>.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     listen_addresses = '*'
     archive_mode = on
     archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
     max_wal_senders = 10
     max_replication_slots = 10
     wal_level = replica
     hot_standby = on
     wal_log_hints = on
    </PRE
><P
>     We use the online recovery functionality of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to setup standby server after the primary server is started.
    </P
></LI
><LI
><P
>     Because of the security reasons, we create a user <TT
CLASS="LITERAL"
>repl</TT
> solely used
     for replication purpose, and a user <TT
CLASS="LITERAL"
>pgpool</TT
> for streaming 
     replication delay check and health check of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. 
    </P
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-USER"
></A
><P
><B
>Table 8-5. Users</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>User Name</TH
><TH
>Password</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>repl</TD
><TD
>repl</TD
><TD
>PostgreSQL replication user</TD
></TR
><TR
><TD
>pgpool</TD
><TD
>pgpool</TD
><TD
>Pgpool-II health check and replication delay check user</TD
></TR
><TR
><TD
>postgres</TD
><TD
>postgres</TD
><TD
>User running online recovery</TD
></TR
></TBODY
></TABLE
></DIV
><PRE
CLASS="PROGRAMLISTING"
>     [server1]# psql -U postgres -p 5432
     postgres=# SET password_encryption = 'scram-sha-256';
     postgres=# CREATE ROLE pgpool WITH LOGIN;
     postgres=# CREATE ROLE repl WITH REPLICATION LOGIN;
     postgres=# \password pgpool
     postgres=# \password repl
     postgres=# \password postgres
    </PRE
><P
>     If you want to show "replication_state" and "replication_sync_state" column in
     <A
HREF="sql-show-pool-nodes.html"
>SHOW POOL NODES</A
> command result, role <TT
CLASS="LITERAL"
>pgpool</TT
>
      needs to be PostgreSQL super user or or in <TT
CLASS="LITERAL"
>pg_monitor</TT
> group 
      (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 or later). Grant <TT
CLASS="LITERAL"
>pg_monitor</TT
>
      to <TT
CLASS="LITERAL"
>pgpool</TT
>:
    </P
><PRE
CLASS="PROGRAMLISTING"
>     GRANT pg_monitor TO pgpool;
    </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>      If you plan to use <A
HREF="runtime-config-failover.html#GUC-DETACH-FALSE-PRIMARY"
>detach_false_primary</A
>(<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 or later),
       role "pgpool" needs to be <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> super user or
       or in "pg_monitor" group to use this feature.
     </P
></BLOCKQUOTE
></DIV
><P
>     Assuming that all the <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers and the 
     <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers are in the same subnet and edit <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> to 
     enable <TT
CLASS="LITERAL"
>scram-sha-256</TT
> authentication method.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     host    all             all             samenet                 scram-sha-256
     host    replication     all             samenet                 scram-sha-256
    </PRE
></LI
><LI
><P
>     To use the automated failover and online recovery of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, 
     the settings that allow <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>passwordless</I
></SPAN
> SSH to all backend servers
     between <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> execution user (default root user)
     and <TT
CLASS="LITERAL"
>postgres</TT
> user and between <TT
CLASS="LITERAL"
>postgres</TT
> user
     and <TT
CLASS="LITERAL"
>postgres</TT
> user are necessary. Execute the following command on all servers
     to set up passwordless <TT
CLASS="LITERAL"
>SSH</TT
>. The generated key file name is <TT
CLASS="LITERAL"
>id_rsa_pgpool</TT
>.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [all servers]# cd ~/.ssh
     [all servers]# ssh-keygen -t rsa -f id_rsa_pgpool
     [all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
     [all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
     [all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3

     [all servers]# su - postgres
     [all servers]$ cd ~/.ssh
     [all servers]$ ssh-keygen -t rsa -f id_rsa_pgpool
     [all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
     [all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
     [all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server3
    </PRE
><P
>     After setting, use <TT
CLASS="COMMAND"
>ssh postgres@serverX -i ~/.ssh/id_rsa_pgpool</TT
> command to
     make sure that you can log in without entering a password. Edit <TT
CLASS="FILENAME"
>/etc/ssh/sshd_config</TT
>
     if necessary and restart sshd.
    </P
></LI
><LI
><P
>     To allow <TT
CLASS="LITERAL"
>repl</TT
> user without specifying password for streaming 
     replication and online recovery, and execute <SPAN
CLASS="APPLICATION"
>pg_rewind</SPAN
>
     using <TT
CLASS="LITERAL"
>postgres</TT
>, we create the <TT
CLASS="FILENAME"
>.pgpass</TT
> file 
     in <TT
CLASS="LITERAL"
>postgres</TT
> user's home directory and change the permission to
     <TT
CLASS="LITERAL"
>600</TT
> on each <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [all servers]# su - postgres
     [all servers]$ vi /var/lib/pgsql/.pgpass
     server1:5432:replication:repl:&lt;repl user password&gt;
     server2:5432:replication:repl:&lt;repl user passowrd&gt;
     server3:5432:replication:repl:&lt;repl user passowrd&gt;
     server1:5432:postgres:postgres:&lt;postgres user passowrd&gt;
     server2:5432:postgres:postgres:&lt;postgres user passowrd&gt;
     server3:5432:postgres:postgres:&lt;postgres user passowrd&gt;
     [all servers]$ chmod 600  /var/lib/pgsql/.pgpass
    </PRE
></LI
><LI
><P
>     When connect to <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers, the target port must be accessible by enabling firewall management softwares. Following is an example for <SPAN
CLASS="SYSTEMITEM"
>CentOS/RHEL7</SPAN
>.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     [all servers]# firewall-cmd --permanent --zone=public --add-service=postgresql
     [all servers]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/udp
     [all servers]# firewall-cmd --reload
    </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG"
>8.3.5. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> Configuration</A
></H2
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-COMMON"
>8.3.5.1. Common Settings</A
></H3
><P
>    Here are the common settings on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
> and <TT
CLASS="LITERAL"
>server3</TT
>.
   </P
><P
>    When installing <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> from RPM,  all the 
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> configuration files are in <TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>. 
    In this example, we copy the sample configuration file for streaming replication mode.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # cp -p /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
   </PRE
><P
>    To allow Pgpool-II to accept all incoming connections, we set <TT
CLASS="VARNAME"
>listen_addresses = '*'</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    listen_addresses = '*'
   </PRE
><P
>    Specify replication delay check user and password. In this example, we leave 
    <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
> empty, and create the entry in <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>.  
      From <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0, if these parameters are left blank, 
      <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> will first try to get the password for that 
      specific user from <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
> file before using the 
       empty password. 
   </P
><PRE
CLASS="PROGRAMLISTING"
>    sr_check_user = 'pgpool'
    sr_check_password = ''
   </PRE
><P
>    Enable health check so that <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> performs failover. Also, if the network is unstable, 
    the health check fails even though the backend is running properly, failover or degenerate operation may occur. 
    In order to prevent such incorrect detection of health check, we set <TT
CLASS="VARNAME"
>health_check_max_retries = 3</TT
>.
    Specify <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-USER"
>health_check_user</A
> and <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
> in
      the same way like <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
> and <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    health_check_period = 5
                                            # Health check period
                                            # Disabled (0) by default
    health_check_timeout = 30
                                            # Health check timeout
                                            # 0 means no timeout
    health_check_user = 'pgpool'
    health_check_password = ''

    health_check_max_retries = 3
   </PRE
><P
>    Specify the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> backend information.
    Multiple backends can be specified by adding a number at the end of the parameter name.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # - Backend Connection Settings -

    backend_hostname0 = 'server1'
                                            # Host name or IP address to connect to for backend 0
    backend_port0 = 5432
                                            # Port number for backend 0
    backend_weight0 = 1
                                            # Weight for backend 0 (only in load balancing mode)
    backend_data_directory0 = '/var/lib/pgsql/11/data'
                                            # Data directory for backend 0
    backend_flag0 = 'ALLOW_TO_FAILOVER'
                                            # Controls various backend behavior
                                            # ALLOW_TO_FAILOVER or DISALLOW_TO_FAILOVER
    backend_hostname1 = 'server2'
    backend_port1 = 5432
    backend_weight1 = 1
    backend_data_directory1 = '/var/lib/pgsql/11/data'
    backend_flag1 = 'ALLOW_TO_FAILOVER'

    backend_hostname2 = 'server3'
    backend_port2 = 5432
    backend_weight2 = 1
    backend_data_directory2 = '/var/lib/pgsql/11/data'
    backend_flag2 = 'ALLOW_TO_FAILOVER'
   </PRE
><P
>    To show "replication_state" and "replication_sync_state" column in <A
HREF="sql-show-pool-nodes.html"
>SHOW POOL NODES</A
>
     command result, <A
HREF="runtime-config-backend-settings.html#GUC-BACKEND-APPLICATION-NAME"
>backend_application_name</A
> parameter is required.
      Here we specify each backend's hostname in these parameters. (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 or later)
   </P
><PRE
CLASS="PROGRAMLISTING"
>    ...
    backend_application_name0 = 'server1'
    ...
    backend_application_name1 = 'server2'
    ...
    backend_application_name2 = 'server3'
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-FAILOVER"
>8.3.5.2. Failover configuration</A
></H3
><P
>    Specify failover.sh script to be executed after failover in <TT
CLASS="VARNAME"
>failover_command</TT
>
    parameter. 
    If we use 3 PostgreSQL servers, we need to specify follow_primary_command to run after failover on the primary node failover.
    In case of two PostgreSQL servers, follow_primary_command setting is not necessary.
   </P
><P
>    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> replaces the following special characters with the backend specific
    information while executing the scripts. 
    See <A
HREF="runtime-config-failover.html#GUC-FAILOVER-COMMAND"
>failover_command</A
> for more details about each character.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
    follow_primary_command = '/etc/pgpool-II/follow_primary.sh %d %h %p %D %m %H %M %P %r %R'
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>%N</I
></SPAN
> and <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>%S</I
></SPAN
> are added in <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1.
     Please note that these characters cannot be specified if using Pgpool-II 4.0 or earlier.
    </P
></BLOCKQUOTE
></DIV
><P
>    Sample scripts <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/failover.sh.sample;hb=refs/heads/V4_1_STABLE"
TARGET="_top"
>failover.sh</A
>
    and <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/follow_primary.sh.sample;hb=refs/heads/V4_1_STABLE"
TARGET="_top"
>follow_primary.sh</A
>
    are installed in <TT
CLASS="FILENAME"
>/etc/pgpool-II/</TT
>. Create failover scripts using these sample files.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # cp /etc/pgpool-II/failover.sh{.sample,}
    # cp /etc/pgpool-II/follow_primary.sh{.sample,}
   </PRE
><P
>    Basically, it should work if you change <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>PGHOME</I
></SPAN
> according to PostgreSQL installation directory.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# vi /etc/pgpool-II/failover.sh
    ...
    PGHOME=/usr/pgsql-11
    ...

    [server1]# vi /etc/pgpool-II/follow_primary.sh
    ...
    PGHOME=/usr/pgsql-11
    ...
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-ONLINE-RECOVERY"
>8.3.5.3. Pgpool-II Online Recovery Configurations</A
></H3
><P
>    Next, in order to perform online recovery with <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> we specify 
    the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> user name and online recovery command 
    <TT
CLASS="COMMAND"
>recovery_1st_stage</TT
>. 
    Because <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>Superuser</I
></SPAN
> privilege in <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>
    is required for performing online recovery, we specify <TT
CLASS="LITERAL"
>postgres</TT
> user in <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-USER"
>recovery_user</A
>.
     Then, we create <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> 
     in database cluster directory of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> primary server (server1), and add execute permission.

   </P
><PRE
CLASS="PROGRAMLISTING"
>    recovery_user = 'postgres'
                                            # Online recovery user
    recovery_password = ''
                                            # Online recovery password

    recovery_1st_stage_command = 'recovery_1st_stage'
   </PRE
><P
>    Online recovery sample scripts<A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/recovery_1st_stage.sample;hb=refs/heads/V4_1_STABLE"
TARGET="_top"
>recovery_1st_stage</A
>
    and <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/pgpool_remote_start.sample;hb=refs/heads/V4_1_STABLE"
TARGET="_top"
>pgpool_remote_start</A
>
    are installed in <TT
CLASS="FILENAME"
>/etc/pgpool-II/</TT
>. Copy these files to the data directory of the primary server (server1).
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# cp /etc/pgpool-II/recovery_1st_stage.sample /var/lib/pgsql/11/data/
    [server1]# cp /etc/pgpool-II/pgpool_remote_start.sample /var/lib/pgsql/11/data/
    [server1]# chown postgres:postgres /var/lib/pgsql/11/data/{recovery_1st_stage,pgpool_remote_start}
   </PRE
><P
>    Basically, it should work if you change <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>PGHOME</I
></SPAN
> according to PostgreSQL installation directory.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# vi /var/lib/pgsql/11/data/recovery_1st_stage
    ...
    PGHOME=/usr/pgsql-11
    ...

    [server1]# vi /var/lib/pgsql/11/data/pgpool_remote_start
    ...
    PGHOME=/usr/pgsql-11
    ...
   </PRE
><P
>    In order to use the online recovery functionality, the functions of 
    <CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
>, <CODE
CLASS="FUNCTION"
>pgpool_remote_start</CODE
>, 
    <CODE
CLASS="FUNCTION"
>pgpool_switch_xlog</CODE
> are required, so we need install 
    <CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
> on template1 of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server 
    <TT
CLASS="LITERAL"
>server1</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# su - postgres
    [server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-AUTH"
>8.3.5.4. Client Authentication Configuration</A
></H3
><P
>    Because in the section <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>Before Starting</A
>, 
    we already set <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> authentication method to 
    <ACRONYM
CLASS="ACRONYM"
>scram-sha-256</ACRONYM
>, it is necessary to set a client authentication by 
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to connect to backend nodes. 
    When installing with RPM, the <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> configuration file 
    <TT
CLASS="FILENAME"
>pool_hba.conf</TT
> is in <TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>. 
    By default, pool_hba authentication is disabled, set <TT
CLASS="VARNAME"
>enable_pool_hba = on</TT
> 
    to enable it.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    enable_pool_hba = on
   </PRE
><P
>    The format of <TT
CLASS="FILENAME"
>pool_hba.conf</TT
> file follows very closely PostgreSQL's 
    <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> format. Set <TT
CLASS="LITERAL"
>pgpool</TT
> and <TT
CLASS="LITERAL"
>postgres</TT
> user's authentication method to <TT
CLASS="LITERAL"
>scram-sha-256</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    host    all         pgpool           0.0.0.0/0          scram-sha-256
    host    all         postgres         0.0.0.0/0          scram-sha-256
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     Please note that in <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 only AES encrypted password or clear text password
     can be specified in <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
>, <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>, 
       <A
HREF="runtime-watchdog-config.html#GUC-WD-LIFECHECK-PASSWORD"
>wd_lifecheck_password</A
>, <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-PASSWORD"
>recovery_password</A
> in <TT
CLASS="FILENAME"
>pgpool.conf</TT
>.
    </P
></BLOCKQUOTE
></DIV
><P
>    The default password file name for authentication is <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>.
     To use <TT
CLASS="LITERAL"
>scram-sha-256</TT
> authentication, the decryption key to decrypt the passwords
     is required. We create the <TT
CLASS="LITERAL"
>.pgpoolkey</TT
> file in <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
     start user <TT
CLASS="LITERAL"
>postgres</TT
>'s (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 or later) home directory.
     (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 or before, by default <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
     is started as <TT
CLASS="LITERAL"
>root</TT
>)
     </P><PRE
CLASS="PROGRAMLISTING"
>      [all servers]# su - postgres
      [all servers]$ echo 'some string' &#62; ~/.pgpoolkey
      [all servers]$ chmod 600 ~/.pgpoolkey
     </PRE
><P>
   </P
><P
>    Execute command <TT
CLASS="COMMAND"
>pg_enc -m -k /path/to/.pgpoolkey -u username -p</TT
> to register user
    name and <TT
CLASS="LITERAL"
>AES</TT
> encrypted password in file <TT
CLASS="FILENAME"
>pool_passwd</TT
>.
    If <TT
CLASS="FILENAME"
>pool_passwd</TT
> doesn't exist yet, it will be created in the same directory as
    <TT
CLASS="FILENAME"
>pgpool.conf</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# su - postgres
    [all servers]$ pg_enc -m -k ~/.pgpoolkey -u pgpool -p
    db password: [pgpool user's password]
    [all servers]$ pg_enc -m -k ~/.pgpoolkey -u postgres -p
    db password: [postgres user's passowrd]

    # cat /etc/pgpool-II/pool_passwd 
    pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
    postgres:AESHs/pWL5rtXy2IwuzroHfqg==
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-WATCHDOG"
>8.3.5.5. Watchdog Configuration</A
></H3
><P
>    Enable watchdog functionality on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    use_watchdog = on
   </PRE
><P
>    Specify virtual IP address that accepts connections from clients on 
    <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>. 
    Ensure that the IP address set to virtual IP isn't used yet.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    delegate_IP = '192.168.137.150'
   </PRE
><P
>    To bring up/down the virtual IP and send the ARP requests, we set <A
HREF="runtime-watchdog-config.html#GUC-IF-UP-CMD"
>if_up_cmd</A
>, <A
HREF="runtime-watchdog-config.html#GUC-IF-DOWN-CMD"
>if_down_cmd</A
> and <A
HREF="runtime-watchdog-config.html#GUC-ARPING-CMD"
>arping_cmd</A
>.
    The network interface used in this example is "enp0s8".
    Since root privilege is required to execute <TT
CLASS="VARNAME"
>if_up/down_cmd</TT
> or
    <TT
CLASS="VARNAME"
>arping_cmd</TT
> command, use setuid on these command or allow 
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> startup user, <TT
CLASS="LITERAL"
>postgres</TT
> user (Pgpool-II 4.1 or later) to run <TT
CLASS="COMMAND"
>sudo</TT
> command without a password.
    If installed from RPM, the <TT
CLASS="LITERAL"
>postgres</TT
> user has been configured to run
    <TT
CLASS="COMMAND"
>ip/arping</TT
> via <TT
CLASS="COMMAND"
>sudo</TT
> without a password.
   </P
><PRE
CLASS="PROGRAMLISTING"
>if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev enp0s8'
arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I enp0s8'
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     If "Defaults requiretty" is set in the <TT
CLASS="FILENAME"
>/etc/sudoers</TT
>,
     please ensure that the <SPAN
CLASS="PRODUCTNAME"
>pgpool</SPAN
> startup user can execute the <TT
CLASS="COMMAND"
>if_up_cmd</TT
>, <TT
CLASS="COMMAND"
>if_down_cmd</TT
> and <TT
CLASS="COMMAND"
>arping_cmd</TT
> command without a tty.
    </P
></BLOCKQUOTE
></DIV
><P
>    Set <A
HREF="runtime-watchdog-config.html#GUC-IF-CMD-PATH"
>if_cmd_path</A
> and <A
HREF="runtime-watchdog-config.html#GUC-ARPING-PATH"
>arping_path</A
> according to the
    command path.
    If <TT
CLASS="VARNAME"
>if_up/down_cmd</TT
> or <TT
CLASS="VARNAME"
>arping_cmd</TT
> starts with "/", these parameters will be ignored. 
   </P
><PRE
CLASS="PROGRAMLISTING"
>if_cmd_path = '/sbin'
arping_path = '/usr/sbin'
   </PRE
><P
>    Specify the hostname and port number of each <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> server.
   </P
><P
></P
><UL
><LI
><P
>      <TT
CLASS="LITERAL"
>server1</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      wd_hostname = 'server1'
      wd_port = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server2</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      wd_hostname = 'server2'
      wd_port = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server3</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      wd_hostname = 'server3'
      wd_port = 9000
     </PRE
></LI
></UL
><P
>    Specify the hostname, <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> port number, and watchdog port number of monitored <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers on each <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> server.
   </P
><P
></P
><UL
><LI
><P
>      <TT
CLASS="LITERAL"
>server1</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server2'
                                            # Host name or IP address to connect to for other pgpool 0
                                            # (change requires restart)
      other_pgpool_port0 = 9999
                                            # Port number for other pgpool 0
                                            # (change requires restart)
      other_wd_port0 = 9000
                                            # Port number for other watchdog 0
                                            # (change requires restart)
      other_pgpool_hostname1 = 'server3'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server2</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server1'
                                            # Host name or IP address to connect to for other pgpool 0
                                            # (change requires restart)
      other_pgpool_port0 = 9999
                                            # Port number for other pgpool 0
                                            # (change requires restart)
      other_wd_port0 = 9000
                                            # Port number for other watchdog 0
                                            # (change requires restart)
      other_pgpool_hostname1 = 'server3'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server3</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server1'
                                            # Host name or IP address to connect to for other pgpool 0
                                            # (change requires restart)
      other_pgpool_port0 = 9999
                                            # Port number for other pgpool 0
                                            # (change requires restart)
      other_wd_port0 = 9000
                                            # Port number for other watchdog 0
                                            # (change requires restart)
      other_pgpool_hostname1 = 'server2'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </PRE
></LI
></UL
><P
>    Specify the hostname and port number of destination for sending heartbeat signal 
    on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>.
   </P
><P
></P
><UL
><LI
><P
>      <TT
CLASS="LITERAL"
>server1</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      heartbeat_destination0 = 'server2'
                                            # Host name or IP address of destination 0
                                            # for sending heartbeat signal.
                                            # (change requires restart)
      heartbeat_destination_port0 = 9694
                                            # Port number of destination 0 for sending
                                            # heartbeat signal. Usually this is the
                                            # same as wd_heartbeat_port.
                                            # (change requires restart)
      heartbeat_device0 = ''
                                            # Name of NIC device (such like 'eth0')
                                            # used for sending/receiving heartbeat
                                            # signal to/from destination 0.
                                            # This works only when this is not empty
                                            # and pgpool has root privilege.
                                            # (change requires restart)

      heartbeat_destination1 = 'server3'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server2</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      heartbeat_destination0 = 'server1'
                                            # Host name or IP address of destination 0
                                            # for sending heartbeat signal.
                                            # (change requires restart)
      heartbeat_destination_port0 = 9694
                                            # Port number of destination 0 for sending
                                            # heartbeat signal. Usually this is the
                                            # same as wd_heartbeat_port.
                                            # (change requires restart)
      heartbeat_device0 = ''
                                            # Name of NIC device (such like 'eth0')
                                            # used for sending/receiving heartbeat
                                            # signal to/from destination 0.
                                            # This works only when this is not empty
                                            # and pgpool has root privilege.
                                            # (change requires restart)

      heartbeat_destination1 = 'server3'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''
     </PRE
></LI
><LI
><P
>      <TT
CLASS="LITERAL"
>server3</TT
>
     </P
><PRE
CLASS="PROGRAMLISTING"
>      heartbeat_destination0 = 'server1'
                                            # Host name or IP address of destination 0
                                            # for sending heartbeat signal.
                                            # (change requires restart)
      heartbeat_destination_port0 = 9694
                                            # Port number of destination 0 for sending
                                            # heartbeat signal. Usually this is the
                                            # same as wd_heartbeat_port.
                                            # (change requires restart)
      heartbeat_device0 = ''
                                            # Name of NIC device (such like 'eth0')
                                            # used for sending/receiving heartbeat
                                            # signal to/from destination 0.
                                            # This works only when this is not empty
                                            # and pgpool has root privilege.
                                            # (change requires restart)

      heartbeat_destination1 = 'server2'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''
     </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-SYSCONFIG"
>8.3.5.6. /etc/sysconfig/pgpool Configuration</A
></H3
><P
>    If you want to ignore the <TT
CLASS="FILENAME"
>pgpool_status</TT
> file at startup of 
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, add "- D" to the start option OPTS to 
    <TT
CLASS="FILENAME"
>/etc/sysconfig/pgpool</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# vi /etc/sysconfig/pgpool 
    ...
    OPTS=" -D -n"
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-LOG"
>8.3.5.7. Logging</A
></H3
><P
>    In the example, we output <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>'s log to <TT
CLASS="LITERAL"
>syslog</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    log_destination = 'syslog'
    # Where to log
    # Valid values are combinations of stderr,
    # and syslog. Default to stderr.

    syslog_facility = 'LOCAL1'
    # Syslog local facility. Default to LOCAL0
   </PRE
><P
>    Create <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> log file.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# mkdir /var/log/pgpool-II
    [all servers]# touch /var/log/pgpool-II/pgpool.log
   </PRE
><P
>    Edit config file of syslog <TT
CLASS="FILENAME"
>/etc/rsyslog.conf</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# vi /etc/rsyslog.conf
    ...
    *.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
    LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
   </PRE
><P
>    Setting logrotate same as <TT
CLASS="FILENAME"
>/var/log/messages</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# vi /etc/logrotate.d/syslog
    ...
    /var/log/messages
    /var/log/pgpool-II/pgpool.log
    /var/log/secure
   </PRE
><P
>    Restart rsyslog service.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# systemctl restart rsyslog
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PCP"
>8.3.5.8. PCP Command Configuration</A
></H3
><P
>    Since user authentication is required to use the <TT
CLASS="LITERAL"
>PCP</TT
> command, 
    specify user name and md5 encrypted password in <TT
CLASS="FILENAME"
>pcp.conf</TT
>.
    Here we create the encrypted password for <TT
CLASS="LITERAL"
>pgpool</TT
> user, and add
    "<TT
CLASS="LITERAL"
>username:encrypted password</TT
>" in <TT
CLASS="FILENAME"
>/etc/pgpool-II/pcp.conf</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# echo 'pgpool:'`pg_md5 PCP passowrd` &gt;&gt; /etc/pgpool-II/pcp.conf
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PCPPASS"
>8.3.5.9. .pcppass</A
></H3
><P
>    Since follow_primary_command script has to execute PCP command without entering the
    password, we create <TT
CLASS="FILENAME"
>.pcppass</TT
> in the home directory of 
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> startup user (root user).
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [all servers]# echo 'localhost:9898:pgpool:pgpool' &#62; ~/.pcppass
    [all servers]# chmod 600 ~/.pcppass
   </PRE
><P
>    The settings of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> is completed. 
   </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-START-STOP"
>8.3.6. Starting/Stopping Pgpool-II</A
></H2
><P
>   Next we start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. Before starting 
   <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, please start 
   <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers first. 
   Also, when stopping <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>, it is necessary to 
   stop Pgpool-II first.
  </P
><P
></P
><UL
><LI
><P
>     Starting <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
    </P
><P
>     In section <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>Before Starting</A
>,
     we already set the auto-start of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. To start
     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, restart the whole system or execute the following command.
    </P
><PRE
CLASS="PROGRAMLISTING"
>     # systemctl start pgpool.service
    </PRE
></LI
><LI
><P
>     Stopping <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
    </P
><PRE
CLASS="PROGRAMLISTING"
>     # systemctl stop pgpool.service
    </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-TRY"
>8.3.7. How to use</A
></H2
><P
>   Let's start to use <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. 
   First, let's start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> on <TT
CLASS="LITERAL"
>server1</TT
>, 
   <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
> by using the following command.
  </P
><PRE
CLASS="PROGRAMLISTING"
>   # systemctl start pgpool.service
  </PRE
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-STANDBY"
>8.3.7.1. Set up PostgreSQL standby server</A
></H3
><P
>    First, we should set up <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> standby server by 
    using <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> online recovery functionality. Ensure 
    that <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> 
    scripts used by <TT
CLASS="COMMAND"
>pcp_recovery_node</TT
> command are in database 
    cluster directory of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> primary server (<TT
CLASS="LITERAL"
>server1</TT
>).
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 1
    Password: 
    pcp_recovery_node -- Command Successful

    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 2
    Password: 
    pcp_recovery_node -- Command Successful
   </PRE
><P
>    After executing <TT
CLASS="COMMAND"
>pcp_recovery_node</TT
> command, 
    verify that <TT
CLASS="LITERAL"
>server2</TT
> and <TT
CLASS="LITERAL"
>server3</TT
>
    are started as <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> standby server.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
    1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
    (3 rows)
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-WATCHDOG"
>8.3.7.2. Switching active/standby watchdog</A
></H3
><P
>    Confirm the watchdog status by using <TT
CLASS="COMMAND"
>pcp_watchdog_info</TT
>. The <TT
CLASS="COMMAND"
>Pgpool-II</TT
> server which is started first run as <TT
CLASS="LITERAL"
>LEADER</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # pcp_watchdog_info -h 192.168.137.150 -p 9898 -U pgpool
    Password: 
    3 YES server1:9999 Linux server1 server1

    server1:9999 Linux server1 server1 9999 9000 4 LEADER  #The Pgpool-II server started first becames "LEADER".
    server2:9999 Linux server2 server2 9999 9000 7 STANDBY #run as standby
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY #run as standby
   </PRE
><P
>    Stop active server <TT
CLASS="LITERAL"
>server1</TT
>, then <TT
CLASS="LITERAL"
>server2</TT
> or 
    <TT
CLASS="LITERAL"
>server3</TT
> will be promoted to active server. To stop 
    <TT
CLASS="LITERAL"
>server1</TT
>, we can stop <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 
    service or shutdown the whole system. Here, we stop <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> service.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# systemctl stop pgpool.service

    # pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
    Password: 
    3 YES server2:9999 Linux server2 server2

    server2:9999 Linux server2 server2 9999 9000 4 LEADER     #server2 is promoted to LEADER
    server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN  #server1 is stopped
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY    #server3 runs as STANDBY
   </PRE
><P
>    Start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> (<TT
CLASS="LITERAL"
>server1</TT
>) which we have stopped again,
    and verify that <TT
CLASS="LITERAL"
>server1</TT
> runs as a standby.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]# systemctl start pgpool.service

    [server1]# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
    Password: 
    3 YES server2:9999 Linux server2 server2

    server2:9999 Linux server2 server2 9999 9000 4 LEADER
    server1:9999 Linux server1 server1 9999 9000 7 STANDBY
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-FAILOVER"
>8.3.7.3. Failover</A
></H3
><P
>    First, use <TT
CLASS="COMMAND"
>psql</TT
> to connect to <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> via virtual IP,
    and verify the backend information.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
    1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
    (3 rows)
   </PRE
><P
>    Next, stop primary <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server 
    <TT
CLASS="LITERAL"
>server1</TT
>, and verify automatic failover.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server1]$ pg_ctl -D /var/lib/pgsql/11/data -m immediate stop
   </PRE
><P
>    After stopping <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on <TT
CLASS="LITERAL"
>server1</TT
>,
    failover occurs and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on 
    <TT
CLASS="LITERAL"
>server2</TT
> becomes new primary DB.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
    1       | server2  | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 |                   |                        | 2019-08-06 11:36:03
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:36:15
    (3 rows)
   </PRE
><P
>    <TT
CLASS="LITERAL"
>server3</TT
> is running as standby of new primary <TT
CLASS="LITERAL"
>server2</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    [server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    t

    [server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    f

    [server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select * from pg_stat_replication" -x
    -[ RECORD 1 ]----+------------------------------
    pid              | 11059
    usesysid         | 16392
    usename          | repl
    application_name | server3
    client_addr      | 192.168.137.103
    client_hostname  | 
    client_port      | 48694
    backend_start    | 2019-08-06 11:36:07.479161+09
    backend_xmin     | 
    state            | streaming
    sent_lsn         | 0/75000148
    write_lsn        | 0/75000148
    flush_lsn        | 0/75000148
    replay_lsn       | 0/75000148
    write_lag        | 
    flush_lag        | 
    replay_lag       | 
    sync_priority    | 0
    sync_state       | async
    reply_time       | 2019-08-06 11:42:59.823961+09
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-ONLINE-RECOVERY"
>8.3.7.4. Online Recovery</A
></H3
><P
>    Here, we use <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> online recovery functionality to
    restore <TT
CLASS="LITERAL"
>server1</TT
> (old primary server) as a standby. Before 
    restoring the old primary server, please ensure that 
    <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> scripts 
    exist in database cluster directory of current primary server <TT
CLASS="LITERAL"
>server2</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 0
    Password: 
    pcp_recovery_node -- Command Successful
   </PRE
><P
>    Then verify that <TT
CLASS="LITERAL"
>server1</TT
> is started as a standby.
   </P
><PRE
CLASS="PROGRAMLISTING"
>    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:48:05
    1       | server2  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:36:15
    (3 rows)
   </PRE
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-watchdog.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="example-aws.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Watchdog Configuration Example</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>AWS Configuration Example</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>