<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Pgpool-II + Watchdog Setup Example</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:pgsql-docs@postgresql.org"><LINK
REL="HOME"
TITLE="pgpool-II 4.1devel Documentation"
HREF="index.html"><LINK
REL="UP"
TITLE="Configuration Examples"
HREF="example-configs.html"><LINK
REL="PREVIOUS"
TITLE="Watchdog Configuration Example"
HREF="example-watchdog.html"><LINK
REL="NEXT"
TITLE="AWS Configuration Example"
HREF="example-aws.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stylesheet.css"><META
HTTP-EQUIV="Content-Type"
CONTENT="text/html; charset=ISO-8859-1"><META
NAME="creation"
CONTENT="2019-05-16T06:20:42"></HEAD
><BODY
CLASS="SECT1"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="4"
ALIGN="center"
VALIGN="bottom"
><A
HREF="index.html"
>pgpool-II 4.1devel Documentation</A
></TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
TITLE="Watchdog Configuration Example"
HREF="example-watchdog.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="60%"
ALIGN="center"
VALIGN="bottom"
>Chapter 8. Configuration Examples</TD
><TD
WIDTH="20%"
ALIGN="right"
VALIGN="top"
><A
TITLE="AWS Configuration Example"
HREF="example-aws.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="EXAMPLE-CLUSTER"
>8.3. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> + Watchdog Setup Example</A
></H1
><P
>This section shows an example of streaming replication configuration using <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. In this example, we use 3 <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers to manage <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers to create a robust cluster system and avoid the single point of failure or split brain.
      </P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-REQUIREMENT"
>8.3.1. Requirements</A
></H2
><P
>We assume that all the Pgpool-II servers and the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers are in the same subnet.
        </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-STRUCTURE"
>8.3.2. Cluster System Configuration</A
></H2
><P
>We use 3 servers with CentOS 7.4. Let these servers be <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>. We install <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> and <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> on each server.
        </P
><P
>          <DIV
CLASS="FIGURE"
><A
NAME="AEN5752"
></A
><P
><B
>Figure 8-1. Cluster System Configuration</B
></P
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG
SRC="cluster_40.gif"></P
></DIV
></DIV
>
        </P
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>          The roles of <TT
CLASS="LITERAL"
>Active</TT
>, <TT
CLASS="LITERAL"
>Standy</TT
>, <TT
CLASS="LITERAL"
>Primary</TT
>, <TT
CLASS="LITERAL"
>Standby</TT
> are not fixed and may be changed by further operations.
          </P
></BLOCKQUOTE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-IP"
></A
><P
><B
>Table 8-2. Hostname and IP address</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Hostname</TH
><TH
>IP Address</TH
><TH
>Virtual IP</TH
></TR
></THEAD
><TBODY
><TR
><TD
>server1</TD
><TD
>192.168.137.101</TD
><TD
ROWSPAN="3"
>192.168.137.150</TD
></TR
><TR
><TD
>server2</TD
><TD
>192.168.137.102</TD
></TR
><TR
><TD
>server3</TD
><TD
>192.168.137.103</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-POSTGRESQL-CONFIG"
></A
><P
><B
>Table 8-3. PostgreSQL version and Configuration</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Item</TH
><TH
>Value</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>PostgreSQL Version</TD
><TD
>11.1</TD
><TD
>-</TD
></TR
><TR
><TD
>port</TD
><TD
>5432</TD
><TD
>-</TD
></TR
><TR
><TD
>$PGDATA</TD
><TD
>/var/lib/pgsql/11/data</TD
><TD
>-</TD
></TR
><TR
><TD
>Archive mode</TD
><TD
>on</TD
><TD
>/var/lib/pgsql/archivedir</TD
></TR
><TR
><TD
>Start automatically</TD
><TD
>Disable</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-PGPOOL-CONFIG"
></A
><P
><B
>Table 8-4. Pgpool-II version and Configuration</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Item</TH
><TH
>Value</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>Pgpool-II Version</TD
><TD
>4.0.2</TD
><TD
>-</TD
></TR
><TR
><TD
ROWSPAN="4"
>port</TD
><TD
>9999</TD
><TD
>Pgpool-II accepts connections</TD
></TR
><TR
><TD
>9898</TD
><TD
>PCP process accepts connections</TD
></TR
><TR
><TD
>9000</TD
><TD
>watchdog accepts connections</TD
></TR
><TR
><TD
>9694</TD
><TD
>UDP port for receiving Watchdog's heartbeat signal</TD
></TR
><TR
><TD
>Config file</TD
><TD
>/etc/pgpool-II/pgpool.conf</TD
><TD
>Pgpool-II config file</TD
></TR
><TR
><TD
>Pgpool-II start user</TD
><TD
>root</TD
><TD
>See <A
HREF="tutorial-watchdog-intro.html#TUTORIAL-WATCHDOG-START-STOP"
>Section 2.1.7</A
> to startup Pgpool-II with non-root user</TD
></TR
><TR
><TD
>Running mode</TD
><TD
>streaming replication mode</TD
><TD
>-</TD
></TR
><TR
><TD
>Watchdog</TD
><TD
>on</TD
><TD
>Life check method: heartbeat</TD
></TR
><TR
><TD
>Start automatically</TD
><TD
>Disable</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-INSTALLATION"
>8.3.3. Installation</A
></H2
><P
>In this example, we install <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0.2 and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 11.1 by using RPM packages.
        </P
><P
>        Install <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> by using <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> YUM repository.
      </P
><PRE
CLASS="PROGRAMLISTING"
># yum install https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/pgdg-centos11-11-2.noarch.rpm
# yum install postgresql11 postgresql11-libs postgresql11-devel postgresql11-server
      </PRE
><P
>        Install <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> by using Pgpool-II YUM repository.
      </P
><PRE
CLASS="PROGRAMLISTING"
># yum install http://www.pgpool.net/yum/rpms/4.0/redhat/rhel-7-x86_64/pgpool-II-release-4.0-1.noarch.rpm
# yum install pgpool-II-pg11-*
      </PRE
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PRE-SETUP"
>8.3.4. Before Starting</A
></H2
><P
>      Before you start the configuration process, please check the following prerequisites.
      </P
><P
></P
><UL
><LI
><P
>Set up <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> streaming replication on the primary server. In this example, we use WAL archiving.
          </P
><P
>First, we create the directory <TT
CLASS="FILENAME"
>/var/lib/pgsql/archivedir</TT
> to store <ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
> segments on all servers.
          </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ mkdir /var/lib/pgsql/archivedir
          </PRE
><P
>Then we edit the configuration file <TT
CLASS="FILENAME"
>$PGDATA/postgresql.conf</TT
> on <TT
CLASS="LITERAL"
>server1</TT
> (primary) as follows.
          </P
><PRE
CLASS="PROGRAMLISTING"
>listen_addresses = '*'
archive_mode = on
archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
          </PRE
><P
>We use the online recovery functionality of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to setup standby server after the primary server is started.
          </P
></LI
><LI
><P
>Because of the security reasons, we create a user <TT
CLASS="LITERAL"
>repl</TT
> solely used
for replication purpose, and a user <TT
CLASS="LITERAL"
>pgpool</TT
> for streaming 
replication delay check and health check of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. 
          </P
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-USER"
></A
><P
><B
>Table 8-5. Users</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>User Name</TH
><TH
>Passowrd</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>repl</TD
><TD
>repl</TD
><TD
>PostgreSQL replication user</TD
></TR
><TR
><TD
>pgpool</TD
><TD
>pgpool</TD
><TD
>Pgpool-II health check and replication delay check user</TD
></TR
><TR
><TD
>postgres</TD
><TD
>postgres</TD
><TD
>User running online recovery</TD
></TR
></TBODY
></TABLE
></DIV
><PRE
CLASS="PROGRAMLISTING"
>[server1]# psql -U postgres -p 5432
postgres=# SET password_encryption = 'scram-sha-256';
postgres=# CREATE ROLE pgpool WITH LOGIN;
postgres=# CREATE ROLE repl WITH REPLICATION;
postgres=# \password pgpool
postgres=# \password repl
postgres=# \password postgres
          </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>	      If you plan to
	      use <A
HREF="runtime-config-failover.html#GUC-DETACH-FALSE-PRIMARY"
>detach_false_primary</A
>, role
	      "pgpool" needs to
	      be <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> super user or
	      or in "pg_monitor" group to use this feature. To let
	      "pgpool" role be in the group, you can do:
          </P><PRE
CLASS="PROGRAMLISTING"
>GRANT pg_monitor TO pgpool;
	  </PRE
><P>
	    </P
></BLOCKQUOTE
></DIV
><P
>Assuming that all the <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers and the 
<SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers are in the network of 
<TT
CLASS="LITERAL"
>192.168.137.0/24</TT
>, and edit <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> to 
enable <TT
CLASS="LITERAL"
>scram-sha-256</TT
> authentication method.
          </P
><PRE
CLASS="PROGRAMLISTING"
>host    all             pgpool          192.168.137.0/24         scram-sha-256
host    all             all             0.0.0.0/0                scram-sha-256

host    replication     repl            192.168.137.0/24         scram-sha-256
          </PRE
></LI
><LI
><P
>To use the failover and online recovery of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, the settings that allow SSH without passowrd to other servers are necessary.
          </P
></LI
><LI
><P
>To allow <TT
CLASS="LITERAL"
>repl</TT
> user without specifying password for streaming 
replication and online recovery, we create the <TT
CLASS="FILENAME"
>.pgpass</TT
> file 
in <TT
CLASS="LITERAL"
>postgres</TT
> user's home directory and change the permisson  to 
<TT
CLASS="LITERAL"
>600</TT
> on each <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server.
          </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ vi /var/lib/pgsql/.pgpass
server1:5432:replication:repl:&lt;repl user password&gt;
server2:5432:replication:repl:&lt;repl user passowrd&gt;
server3:5432:replication:repl:&lt;repl user passowrd&gt;
[all servers]$ chmod 600  /var/lib/pgsql/.pgpass
            </PRE
></LI
><LI
><P
>When connect to <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers, the target port must be accessible by enabling firewall management softwares. Following is an example for <SPAN
CLASS="SYSTEMITEM"
>CentOS/RHEL7</SPAN
>.
          </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# firewall-cmd --permanent --zone=public --add-service=postgresql
[all servers]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/tcp
[all servers]# firewall-cmd --permanent --zone=public --add-port=9999/udp --add-port=9898/udp --add-port=9000/udp  --add-port=9694/udp
[all servers]# firewall-cmd --reload
            </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG"
>8.3.5. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> Configuration</A
></H2
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-COMMON"
>8.3.5.1. Common Settings</A
></H3
><P
>Here are the common settings on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
> and <TT
CLASS="LITERAL"
>server3</TT
>.
        </P
><P
>When installing <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> from RPM,  all the 
<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> configuration files are in <TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>. 
In this example, we copy the sample configuration file for streaming replicaton mode.
        </P
><PRE
CLASS="PROGRAMLISTING"
># cp /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
        </PRE
><P
>To allow Pgpool-II to accept all incoming connections, we set <TT
CLASS="VARNAME"
>listen_addresses = '*'</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>listen_addresses = '*'
        </PRE
><P
>Specifiy replication delay check user and password. In this example, we leave 
<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
> empty, and create the entry in <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>.  
From <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0, if these parameters are left blank, 
<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> will first try to get the password for that 
specific user from <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
> file before using the 
empty password. 
        </P
><PRE
CLASS="PROGRAMLISTING"
>sr_check_user = 'pgpool'
sr_check_password = ''
        </PRE
><P
>Enable health check so that pgpool-II performs failover. Also, if the network is unstable, 
the health check fails even though the backend is running properly, failover or degenerate operation may occur. 
In order to prevent such incorrect detection of health check, we set <TT
CLASS="VARNAME"
>health_check_max_retries = 3</TT
>.
Specify <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-USER"
>health_check_user</A
> and <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
> in the same way like <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
> and <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>health_check_period = 5
                                   # Health check period
                                   # Disabled (0) by default
health_check_timeout = 30
                                   # Health check timeout
                                   # 0 means no timeout
health_check_user = 'pgpool'
health_check_password = ''

health_check_max_retries = 3
        </PRE
><P
>Specify the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> backend informations.
Multiple backends can be specified by adding a number at the end of the parameter name.
        </P
><PRE
CLASS="PROGRAMLISTING"
># - Backend Connection Settings -

backend_hostname0 = 'server1'
                                   # Host name or IP address to connect to for backend 0
backend_port0 = 5432
                                   # Port number for backend 0
backend_weight0 = 1
                                   # Weight for backend 0 (only in load balancing mode)
backend_data_directory0 = '/var/lib/pgsql/11/data'
                                   # Data directory for backend 0
backend_flag0 = 'ALLOW_TO_FAILOVER'
                                   # Controls various backend behavior
                                   # ALLOW_TO_FAILOVER or DISALLOW_TO_FAILOVER
backend_hostname1 = 'server2'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/var/lib/pgsql/11/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'

backend_hostname2 = 'server3'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/var/lib/pgsql/11/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-FAILOVER"
>8.3.5.2. Failover configuration</A
></H3
><P
>Specify failover.sh script to be executed after failover in <TT
CLASS="VARNAME"
>failover_command</TT
>
parameter. 
If we use 3 PostgreSQL servers, we need to specify follow_master_command to run after failover on the primary node failover.
In case of two PostgreSQL servers, follow_master_command setting is not necessary.
        </P
><PRE
CLASS="PROGRAMLISTING"
>failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R'
follow_master_command = '/etc/pgpool-II/follow_master.sh %d %h %p %D %m %M %H %P %r %R'
      </PRE
><P
>Create <TT
CLASS="FILENAME"
>/etc/pgpool-II/failover.sh</TT
>, and add execute permission.
      </P
><PRE
CLASS="PROGRAMLISTING"
># vi /etc/pgpool-II/failover.sh
# vi /etc/pgpool-II/follow_master.sh
# chmod +x /etc/pgpool-II/{failover.sh,follow_master.sh}
      </PRE
><P
></P
><UL
><LI
><P
>/etc/pgpool-II/failover.sh
          </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run by failover_command.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

# Special values:
#   %d = node id
#   %h = host name
#   %p = port number
#   %D = database cluster path
#   %m = new master node id
#   %H = hostname of the new master node
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/pgsql-11

logger -i -p local1.info failover.sh: start: failed_node_id=$FAILED_NODE_ID old_primary_node_id=$OLD_PRIMARY_NODE_ID \
	failed_host=$FAILED_NODE_HOST new_master_host=$NEW_MASTER_NODE_HOST

# If standby node is down, skip failover.
if [ $FAILED_NODE_ID -ne $OLD_PRIMARY_NODE_ID ]; then
    logger -i -p local1.info failover.sh: Standby node is down. Skipping failover.
    exit 0
fi

# Promote standby node.
logger -i -p local1.info failover.sh: ssh: postgres@$NEW_MASTER_NODE_HOST pg_ctl promote
if [ $UID -eq 0 ]; then
    su postgres -c "ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@$NEW_MASTER_NODE_HOST ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote"
else
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@$NEW_MASTER_NODE_HOST ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote
fi

if [[ $? -ne 0 ]]; then
    logger -i -p local1.error failover.sh: new_master_host=$NEW_MASTER_NODE_HOST promote failed
    exit 1
fi

logger -i -p local1.info failover.sh: end: new_master_node_id=$NEW_MASTER_NODE_ID started as the primary node
exit 0
          </PRE
></LI
></UL
><P
></P
><UL
><LI
><P
>/etc/pgpool-II/follow_master.sh
          </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run after failover_command to recover the slave from the new primary.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

# special values:  %d = node id
#                  %h = host name
#                  %p = port number
#                  %D = database cluster path
#                  %m = new master node id
#                  %M = old master node id
#                  %H = new master node host name
#                  %P = old primary node id
#                  %R = new master database cluster path
#                  %r = new master port number
#                  %% = '%' character
FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
OLD_MASTER_NODE_ID="$6"
NEW_MASTER_NODE_HOST="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPL_USER=repl
PCP_USER=pgpool
PGPOOL_PATH=/usr/bin
PCP_PORT=9898


# Recovery the slave from the new primary
logger -i -p local1.info follow_master.sh: start: pg_basebackup for $FAILED_NODE_ID

# Check the status of standby
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@${FAILED_NODE_HOST} ${PGHOME}/bin/pg_ctl -w -D ${FAILED_NODE_PGDATA} status &#62;/dev/null 2&#62;&#38;1

# If slave is running, recover the slave from the new primary.
if [[ $? -eq 0 ]]; then

    # Execute pg_basebackup at slave
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} "
        ${PGHOME}/bin/pg_ctl -w -m f -D ${FAILED_NODE_PGDATA} stop

        rm -rf ${FAILED_NODE_PGDATA}
        ${PGHOME}/bin/pg_basebackup -h ${NEW_MASTER_NODE_HOST} -U ${REPL_USER} -p ${NEW_MASTER_NODE_PORT} -D ${FAILED_NODE_PGDATA} -X stream -R

        if [[ $? -ne 0 ]]; then
            logger -i -p local1.error follow_master.sh: end: pg_basebackup failed
            exit 1
        fi
        rm -rf ${ARCHIVEDIR}/*
cat &gt;&gt; ${FAILED_NODE_PGDATA}/recovery.conf &lt;&lt; EOT
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
EOT
        $PGHOME/bin/pg_ctl -l /dev/null -w -D ${FAILED_NODE_PGDATA} start
    "

    if [[ $? -eq 0 ]]; then

        # Run pcp_attact_node to attach this slave to Pgpool-II.
        ${PGPOOL_PATH}/pcp_attach_node -w -h localhost -U ${PCP_USER} -p ${PCP_PORT} -n ${FAILED_NODE_ID}

        if [[ $? -ne 0 ]]; then
            logger -i -p local1.error follow_master.sh: end: pcp_attach_node failed
            exit 1
        else
            logger -i -p local1.error follow_master.sh: end: follow master failed
            exit 1
        fi

else
    logger -i -p local1.info follow_master.sh: failed_nod_id=${FAILED_NODE_ID} is not running. skipping follow master command.
    exit 0
fi

logger -i -p local1.info follow_master.sh: end: follow master is finished
exit 0
          </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-ONLINE-RECOVERY"
>8.3.5.3. Pgpool-II Online Recovery Configurations</A
></H3
><P
>Next, in order to perform online recovery with <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> we specify 
the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> user name and online recovery command 
<TT
CLASS="COMMAND"
>recovery_1st_stage</TT
>. 
Because Supergroup privilege of PostgreSQL is required for online recovery, we specify postgres user to <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-USER"
>recovery_user</A
>.
Then, we create <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> 
in database cluster directory of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> primary server (server1), and add execute permission.

        </P
><PRE
CLASS="PROGRAMLISTING"
>recovery_user = 'postgres'
                                   # Online recovery user
recovery_password = ''
                                   # Online recovery password

recovery_1st_stage_command = 'recovery_1st_stage'
        </PRE
><PRE
CLASS="PROGRAMLISTING"
>[server1]# su - postgres
[server1]$ vi /var/lib/pgsql/11/data/recovery_1st_stage
[server1]$ vi /var/lib/pgsql/11/data/pgpool_remote_start
[server1]$ chmod +x /var/lib/pgsql/11/data/{recovery_1st_stage,pgpool_remote_start}
        </PRE
><P
></P
><UL
><LI
><P
>/var/lib/pgsql/11/data/recovery_1st_stage
            </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run by recovery_1st_stage to recovery the slave from the primary.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

PRIMARY_NODE_PGDATA="$1"
DEST_NODE_HOST="$2"
DEST_NODE_PGDATA="$3"
PRIMARY_NODE_PGPORT=$4

PRIMARY_NODE_HOST=$(hostname -s)
PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPLUSER=repl


logger -i -p local1.info online_recovery.sh: start: pg_basebackup for $DEST_NODE_HOST

# Run pg_basebackup to recovery the slave from the primary
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST "
rm -rf $DEST_NODE_PGDATA
${PGHOME}/bin/pg_basebackup -h $PRIMARY_NODE_HOST -U $REPLUSER -p $PRIMARY_NODE_PGPORT -D $DEST_NODE_PGDATA -X stream -R
"
if [[ $? -ne 0 ]]; then
    logger -i -p local1.error online_recovery.sh: end: pg_basebackup failed. online recovery failed.
    exit 1
fi

ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST "
rm -rf $ARCHIVEDIR/*
cat &gt;&gt; $DEST_NODE_PGDATA/recovery.conf &lt;&lt; EOT
restore_command = 'scp $PRIMARY_NODE_HOST:$archivedir/%f %p'
EOT
"

if [[ $? -ne 0 ]]; then
    logger -i -p local1.error online_recovery.sh: end: online recovery failed
    exit 1
else
    logger -i -p local1.info online_recovery.sh: end: online recovery is finished
    exit 0
fi
            </PRE
></LI
><LI
><P
>/var/lib/pgsql/11/data/pgpool_remote_start
          </P
><PRE
CLASS="PROGRAMLISTING"
>#!/bin/bash
# This script is run to start slave node after recovery.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&#38;1

PGHOME=/usr/pgsql-11
DEST_HOST="$1"
DEST_HOST_PGDATA="$2"


logger -i -p local1.info pgpool_remote_start: start: remote start PostgreSQL@$DEST_HOST

# Start slave node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_HOST $PGHOME/bin/pg_ctl -l /dev/null -w -D $DEST_HOST_PGDATA start

if [[ $? -ne 0 ]]; then
    logger -i -p local1.info  pgpool_remote_start: $DEST_HOST start failed.
    exit 1
fi

logger -i -p local1.info pgpool_remote_start: end: $DEST_HOST PostgreSQL started successfully.
          </PRE
></LI
></UL
><P
>        In order to use the online recovery functionality, the functions of 
        <CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
>, <CODE
CLASS="FUNCTION"
>pgpool_remote_start</CODE
>, 
        <CODE
CLASS="FUNCTION"
>pgpool_switch_xlog</CODE
> are required, so we need install 
        <CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
> on template1 of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server 
        <TT
CLASS="LITERAL"
>server1</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# su - postgres
[server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-AUTH"
>8.3.5.4. Client Authentication Configuration</A
></H3
><P
>Because in the section <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>Before Starting</A
>, 
we already set <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> authentication method to 
<ACRONYM
CLASS="ACRONYM"
>scram-sha-256</ACRONYM
>, it is necessary to set a client authentication by 
<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to connect to backend nodes. 
Please note that only AES encrypted password or clear text passowrd can be specified
in <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
>, <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>, 
<A
HREF="runtime-watchdog-config.html#GUC-WD-LIFECHECK-PASSWORD"
>wd_lifecheck_password</A
>, <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-PASSWORD"
>recovery_password</A
> in <TT
CLASS="FILENAME"
>pgpool.conf</TT
>.
When installing with RPM, 
the <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> configuration file <TT
CLASS="FILENAME"
>pool_hba.conf</TT
> 
is in <TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>. 
By default, pool_hba authentication is disabled, and set <TT
CLASS="VARNAME"
>enable_pool_hba = on</TT
> 
to enable it.
        </P
><PRE
CLASS="PROGRAMLISTING"
>enable_pool_hba = on
        </PRE
><P
>The format of <TT
CLASS="FILENAME"
>pool_hba.conf</TT
> file follows very closely PostgreSQL's 
<TT
CLASS="FILENAME"
>pg_hba.conf</TT
> format. Set <TT
CLASS="LITERAL"
>pgpool</TT
> and <TT
CLASS="LITERAL"
>postgres</TT
> user's authentication method to <TT
CLASS="LITERAL"
>scram-sha-256</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>host    all         pgpool           0.0.0.0/0          scram-sha-256
host    all         postgres         0.0.0.0/0          scram-sha-256
        </PRE
><P
>The default password file name for authentication is <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>.
To use <TT
CLASS="LITERAL"
>scram-sha-256</TT
> authentication, the decryption key to decrypt the passwords
is required. We create the .pgpoolkey file in root user's home directory.
        </P><PRE
CLASS="PROGRAMLISTING"
>[all servers]# echo 'some string' &#62; ~/.pgpoolkey 
[all servers]# chmod 600 ~/.pgpoolkey
        </PRE
><P>
        </P
><P
>Execute command <TT
CLASS="COMMAND"
>pg_enc -m -k /path/to/.pgpoolkey -u username -p</TT
> to regist user 
name and <TT
CLASS="LITERAL"
>AES</TT
> encrypted password in file <TT
CLASS="FILENAME"
>pool_passwd</TT
>. 
If <TT
CLASS="FILENAME"
>pool_passwd</TT
> doesn't exist yet, it will be created in the same directory as
<TT
CLASS="FILENAME"
>pgpool.conf</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# pg_enc -m -k /root/.pgpoolkey -u pgpool -p
db password: [pgpool user's password]
[all servers]# pg_enc -m -k /root/.pgpoolkey -u postgres -p
db password: [postgres user's passowrd]

# cat /etc/pgpool-II/pool_passwd 
pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
postgres:AESHs/pWL5rtXy2IwuzroHfqg==
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-WATCHDOG"
>8.3.5.5. Watchdog Configuration</A
></H3
><P
>        Enable watchdog functionality on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>use_watchdog = on
        </PRE
><P
>        Specify virtual IP address that accepts connections from clients on 
		<TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>. 
		Ensure that the IP address set to virtual IP isn't used yet.
        </P
><PRE
CLASS="PROGRAMLISTING"
>delegate_IP = '192.168.137.150'
        </PRE
><P
>		To bring up/down the virtual IP and send the ARP requests, we set <A
HREF="runtime-watchdog-config.html#GUC-IF-UP-CMD"
>if_up_cmd</A
>, <A
HREF="runtime-watchdog-config.html#GUC-IF-DOWN-CMD"
>if_down_cmd</A
> and <A
HREF="runtime-watchdog-config.html#GUC-ARPING-CMD"
>arping_cmd</A
>.
		The network interface used in this example is "enp0s8".
        </P
><PRE
CLASS="PROGRAMLISTING"
>if_up_cmd = 'ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
                                    # startup delegate IP command
if_down_cmd = 'ip addr del $_IP_$/24 dev enp0s8'
                                    # shutdown delegate IP command
arping_cmd = 'arping -U $_IP_$ -w 1 -I enp0s8'
                                    # arping command
        </PRE
><P
>		Set <A
HREF="runtime-watchdog-config.html#GUC-IF-CMD-PATH"
>if_cmd_path</A
> and <A
HREF="runtime-watchdog-config.html#GUC-ARPING-PATH"
>arping_path</A
> according to the
		command path.
        </P
><PRE
CLASS="PROGRAMLISTING"
>if_cmd_path = '/sbin'
                                    # path to the directory where if_up/down_cmd exists
arping_path = '/usr/sbin'
                                    # arping command path
        </PRE
><P
>         Specify the hostname and port number of each <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> server.
        </P
><P
></P
><UL
><LI
><P
>              <TT
CLASS="LITERAL"
>server1</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
>wd_hostname = 'server1'
wd_port = 9000
            </PRE
></LI
><LI
><P
>              <TT
CLASS="LITERAL"
>server2</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
>wd_hostname = 'server2'
wd_port = 9000
            </PRE
></LI
><LI
><P
>              <TT
CLASS="LITERAL"
>server3</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
>wd_hostname = 'server3'
wd_port = 9000
            </PRE
></LI
></UL
><P
>          Specify the hostname, <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> port number, and watchdog port number of monitored <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers on each <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> server.
        </P
><P
></P
><UL
><LI
><P
>              <TT
CLASS="LITERAL"
>server1</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
># - Other pgpool Connection Settings -

other_pgpool_hostname0 = 'server2'
                                    # Host name or IP address to connect to for other pgpool 0
                                    # (change requires restart)
other_pgpool_port0 = 9999
                                    # Port number for other pgpool 0
                                    # (change requires restart)
other_wd_port0 = 9000
                                    # Port number for other watchdog 0
                                    # (change requires restart)
other_pgpool_hostname1 = 'server3'
other_pgpool_port1 = 9999
other_wd_port1 = 9000
            </PRE
></LI
><LI
><P
>              <TT
CLASS="LITERAL"
>server2</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
># - Other pgpool Connection Settings -

other_pgpool_hostname0 = 'server1'
                                    # Host name or IP address to connect to for other pgpool 0
                                    # (change requires restart)
other_pgpool_port0 = 9999
                                    # Port number for other pgpool 0
                                    # (change requires restart)
other_wd_port0 = 9000
                                    # Port number for other watchdog 0
                                    # (change requires restart)
other_pgpool_hostname1 = 'server3'
other_pgpool_port1 = 9999
other_wd_port1 = 9000
            </PRE
></LI
><LI
><P
>              <TT
CLASS="LITERAL"
>server3</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
># - Other pgpool Connection Settings -

other_pgpool_hostname0 = 'server1'
                                    # Host name or IP address to connect to for other pgpool 0
                                    # (change requires restart)
other_pgpool_port0 = 9999
                                    # Port number for other pgpool 0
                                    # (change requires restart)
other_wd_port0 = 9000
                                    # Port number for other watchdog 0
                                    # (change requires restart)
other_pgpool_hostname1 = 'server2'
other_pgpool_port1 = 9999
other_wd_port1 = 9000
            </PRE
></LI
></UL
><P
>          Specify the hostname and port number of destination for sending heartbeat signal 
          on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>.
        </P
><P
></P
><UL
><LI
><P
>              <TT
CLASS="LITERAL"
>server1</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
>heartbeat_destination0 = 'server2'
                                    # Host name or IP address of destination 0
                                    # for sending heartbeat signal.
                                    # (change requires restart)
heartbeat_destination_port0 = 9694
                                    # Port number of destination 0 for sending
                                    # heartbeat signal. Usually this is the
                                    # same as wd_heartbeat_port.
                                    # (change requires restart)
heartbeat_device0 = ''
                                    # Name of NIC device (such like 'eth0')
                                    # used for sending/receiving heartbeat
                                    # signal to/from destination 0.
                                    # This works only when this is not empty
                                    # and pgpool has root privilege.
                                    # (change requires restart)

heartbeat_destination1 = 'server3'
heartbeat_destination_port1 = 9694
heartbeat_device1 = ''

            </PRE
></LI
><LI
><P
>              <TT
CLASS="LITERAL"
>server2</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
>heartbeat_destination0 = 'server1'
                                    # Host name or IP address of destination 0
                                    # for sending heartbeat signal.
                                    # (change requires restart)
heartbeat_destination_port0 = 9694
                                    # Port number of destination 0 for sending
                                    # heartbeat signal. Usually this is the
                                    # same as wd_heartbeat_port.
                                    # (change requires restart)
heartbeat_device0 = ''
                                    # Name of NIC device (such like 'eth0')
                                    # used for sending/receiving heartbeat
                                    # signal to/from destination 0.
                                    # This works only when this is not empty
                                    # and pgpool has root privilege.
                                    # (change requires restart)

heartbeat_destination1 = 'server3'
heartbeat_destination_port1 = 9694
heartbeat_device1 = ''

            </PRE
></LI
><LI
><P
>              <TT
CLASS="LITERAL"
>server3</TT
>
            </P
><PRE
CLASS="PROGRAMLISTING"
>heartbeat_destination0 = 'server1'
                                    # Host name or IP address of destination 0
                                    # for sending heartbeat signal.
                                    # (change requires restart)
heartbeat_destination_port0 = 9694
                                    # Port number of destination 0 for sending
                                    # heartbeat signal. Usually this is the
                                    # same as wd_heartbeat_port.
                                    # (change requires restart)
heartbeat_device0 = ''
                                    # Name of NIC device (such like 'eth0')
                                    # used for sending/receiving heartbeat
                                    # signal to/from destination 0.
                                    # This works only when this is not empty
                                    # and pgpool has root privilege.
                                    # (change requires restart)

heartbeat_destination1 = 'server2'
heartbeat_destination_port1 = 9694
heartbeat_device1 = ''
            </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-SYSCONFIG"
>8.3.5.6. /etc/sysconfig/pgpool Configuration</A
></H3
><P
>         If you want to ignore the <TT
CLASS="FILENAME"
>pgpool_status</TT
> file at startup of 
         <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, add "- D" to the start option OPTS to 
         <TT
CLASS="FILENAME"
>/etc/sysconfig/pgpool</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# vi /etc/sysconfig/pgpool 
...
OPTS=" -D -n"
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-LOG"
>8.3.5.7. Logging</A
></H3
><P
>In the example, we output <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>'s log to <TT
CLASS="LITERAL"
>syslog</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>log_destination = 'syslog'
                                   # Where to log
                                   # Valid values are combinations of stderr,
                                   # and syslog. Default to stderr.

syslog_facility = 'LOCAL1'
                                   # Syslog local facility. Default to LOCAL0
        </PRE
><P
>          Create <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> log file.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# mkdir /var/log/pgpool-II
[all servers]# touch /var/log/pgpool-II/pgpool.log
        </PRE
><P
>         Edit config file of syslog <TT
CLASS="FILENAME"
>/etc/rsyslog.conf</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# vi /etc/rsyslog.conf
...
*.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
        </PRE
><P
>          Setting logrotate same as <TT
CLASS="FILENAME"
>/var/log/messages</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# vi /etc/logrotate.d/syslog
...
/var/log/messages
/var/log/pgpool-II/pgpool.log
/var/log/secure
        </PRE
><P
>         Restart rsyslog service.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# systemctl restart rsyslog
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PCP"
>8.3.5.8. PCP Command Configuration</A
></H3
><P
>         Since user authentication is required to use the <TT
CLASS="LITERAL"
>PCP</TT
> command, 
         specify user name and md5 encrypted password in <TT
CLASS="FILENAME"
>pcp.conf</TT
>.
          Here we create the encrypted password for <TT
CLASS="LITERAL"
>pgpool</TT
> user, and add
          "<TT
CLASS="LITERAL"
>username:encrypted password</TT
>" in <TT
CLASS="FILENAME"
>/etc/pgpool-II/pcp.conf</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# echo 'pgpool:'`pg_md5 PCP passowrd` &#62;&#62; /etc/pgpool-II/pcp.conf
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PCPPASS"
>8.3.5.9. .pcppass</A
></H3
><P
>         Since follow_master_command script has to execute PCP command without entering the
         password, we create <TT
CLASS="FILENAME"
>.pcppass</TT
> in the home directory of 
         <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> startup user (root user).
        </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# echo 'localhost:9898:pgpool:pgpool' &#62; ~/.pcppass
[all servers]# chmod 600 ~/.pcppass
        </PRE
><P
>         The settings of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> is completed. 
        </P
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-START-STOP"
>8.3.6. Starting/Stopping Pgpool-II</A
></H2
><P
>        Next we start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. Before starting 
        <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, please start 
        <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers first. 
        Also, when stopping <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>, it is necessary to 
        stop Pgpool-II first.
      </P
><P
></P
><UL
><LI
><P
>Starting <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
          </P
><P
>            In section <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>Before Starting</A
>, we already set the auto-start of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. To start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, restart the whole system or execute the following command.
          </P
><PRE
CLASS="PROGRAMLISTING"
># systemctl start pgpool.service
          </PRE
></LI
><LI
><P
>            Stopping <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
          </P
><PRE
CLASS="PROGRAMLISTING"
># systemctl stop pgpool.service
          </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-TRY"
>8.3.7. How to use</A
></H2
><P
>        Let's start to use <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. 
        First, let's start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> on <TT
CLASS="LITERAL"
>server1</TT
>, 
        <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
> by using the following command.
      </P
><PRE
CLASS="PROGRAMLISTING"
># systemctl start pgpool.service
      </PRE
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-STANDBY"
>8.3.7.1. Set up PostgreSQL standby server</A
></H3
><P
>         First, we should set up <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> standby server by 
         using <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> online recovery functionality. Ensure 
         that <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> 
         scripts used by <TT
CLASS="COMMAND"
>pcp_recovery_node</TT
> command are in database 
         cluster directory of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> primary server (<TT
CLASS="LITERAL"
>server1</TT
>).
        </P
><PRE
CLASS="PROGRAMLISTING"
># pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 1
Password: 
pcp_recovery_node -- Command Successful

# pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 2
Password: 
pcp_recovery_node -- Command Successful
      </PRE
><P
>        After executing <TT
CLASS="COMMAND"
>pcp_recovery_node</TT
> command, 
        vertify that <TT
CLASS="LITERAL"
>server2</TT
> and <TT
CLASS="LITERAL"
>server3</TT
> 
        are started as <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> standby server.
      </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
 node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
 0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 | 2019-02-18 11:26:31
 1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | 2019-02-18 11:27:49
 2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 11:27:49
      </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-WATCHDOG"
>8.3.7.2. Switching active/standby watchdog</A
></H3
><P
>          Confirm the watchdog status by using <TT
CLASS="COMMAND"
>pcp_watchdog_info</TT
>. The <TT
CLASS="COMMAND"
>Pgpool-II</TT
> server which is started first run as <TT
CLASS="LITERAL"
>MASTER</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
># pcp_watchdog_info -h 192.168.137.150 -p 9898 -U pgpool
Password: 
3 YES server1:9999 Linux server1 server1

server1:9999 Linux server1 server1 9999 9000 4 MASTER  #The Pgpool-II server started first becames "MASTER".
server2:9999 Linux server2 server2 9999 9000 7 STANDBY #run as standby
server3:9999 Linux server3 server3 9999 9000 7 STANDBY #run as standby
        </PRE
><P
>         Stop active server <TT
CLASS="LITERAL"
>server1</TT
>, then <TT
CLASS="LITERAL"
>server2</TT
> or 
         <TT
CLASS="LITERAL"
>server3</TT
> will be promoted to active server. To stop 
         <TT
CLASS="LITERAL"
>server1</TT
>, we can stop <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 
         service or shutdown the whole system. Here, we stop <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> service.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# systemctl stop pgpool.service

# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
Password: 
3 YES server2:9999 Linux server2 server2

server2:9999 Linux server2 server2 9999 9000 4 MASTER     #server2 is promoted to MASTER
server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN  #server1 is stopped
server3:9999 Linux server3 server3 9999 9000 7 STANDBY    #server3 runs as STANDBY
        </PRE
><P
>Start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> (<TT
CLASS="LITERAL"
>server1</TT
>) which we have stopped again, and vertify that <TT
CLASS="LITERAL"
>server1</TT
> runs as a standby.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# systemctl start pgpool.service

[server1]# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
Password: 
3 YES server2:9999 Linux server2 server2

server2:9999 Linux server2 server2 9999 9000 4 MASTER
server1:9999 Linux server1 server1 9999 9000 7 STANDBY
server3:9999 Linux server3 server3 9999 9000 7 STANDBY
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-FAILOVER"
>8.3.7.3. Failover</A
></H3
><P
>First, use <TT
CLASS="COMMAND"
>psql</TT
> to connect to <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> via virtual IP, and verify the backend informations.
        </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
 node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
 0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 | 2019-02-18 13:08:02
 1       | server2  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:21:56
 2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | 2019-02-18 13:21:56
        </PRE
><P
>         Next, stop primary <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server 
         <TT
CLASS="LITERAL"
>server1</TT
>, and verify automatic failover.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]$ pg_ctl -D /var/lib/pgsql/11/data -m immediate stop
        </PRE
><P
>         After stopping <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on <TT
CLASS="LITERAL"
>server1</TT
>,
         failover occurs and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on 
         <TT
CLASS="LITERAL"
>server2</TT
> becomes new primary DB.
        </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
 node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
 0       | server1  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:22:25
 1       | server2  | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 | 2019-02-18 13:22:25
 2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:22:28
        </PRE
><P
><TT
CLASS="LITERAL"
>server3</TT
> is running as standby of new primary <TT
CLASS="LITERAL"
>server2</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
>[server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
 pg_is_in_recovery 
-------------------
 t

[server2]# su - postgres
$ psql
postgres=# select pg_is_in_recovery();
 pg_is_in_recovery 
-------------------
 f

postgres=# select * from pg_stat_replication;
-[ RECORD 1 ]----+------------------------------
pid              | 11915
usesysid         | 16385
usename          | repl
application_name | walreceiver
client_addr      | 192.168.137.103
client_hostname  | 
client_port      | 37834
backend_start    | 2019-02-18 13:22:27.472038+09
backend_xmin     | 
state            | streaming
sent_lsn         | 0/8E000060
write_lsn        | 0/8E000060
flush_lsn        | 0/8E000060
replay_lsn       | 0/8E000060
write_lag        | 
flush_lag        | 
replay_lag       | 
sync_priority    | 0
sync_state       | async
        </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-ONLINE-RECOVERY"
>8.3.7.4. Online Recovery</A
></H3
><P
>         Here, we use <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> online recovery functionality to
         restore <TT
CLASS="LITERAL"
>server1</TT
> (old primary server) as a standby. Before 
         restoring the old primary server, please ensure that 
         <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> scripts 
         exist in database cluster directory of current primary server <TT
CLASS="LITERAL"
>server2</TT
>.
        </P
><PRE
CLASS="PROGRAMLISTING"
># pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 0
Password: 
pcp_recovery_node -- Command Successful
        </PRE
><P
>         Then verify that <TT
CLASS="LITERAL"
>server1</TT
> is started as a standby.
        </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
 node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+---------------------
 0       | server1  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | 2019-02-18 13:27:44
 1       | server2  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 | 2019-02-18 13:22:25
 2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | 2019-02-18 13:22:28
        </PRE
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-watchdog.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="example-aws.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Watchdog Configuration Example</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>AWS Configuration Example</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>