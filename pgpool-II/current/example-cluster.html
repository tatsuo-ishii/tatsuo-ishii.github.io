<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Pgpool-II + Watchdog Setup Example</TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><LINK
REV="MADE"
HREF="mailto:pgsql-docs@postgresql.org"><LINK
REL="HOME"
TITLE="pgpool-II 4.3devel Documentation"
HREF="index.html"><LINK
REL="UP"
TITLE="Configuration Examples"
HREF="example-configs.html"><LINK
REL="PREVIOUS"
TITLE="Basic Configuration Example"
HREF="example-basic.html"><LINK
REL="NEXT"
TITLE="AWS Configuration Example"
HREF="example-aws.html"><LINK
REL="STYLESHEET"
TYPE="text/css"
HREF="stylesheet.css"><META
HTTP-EQUIV="Content-Type"
CONTENT="text/html; charset=ISO-8859-1"><META
NAME="creation"
CONTENT="2021-08-31T07:11:19"></HEAD
><BODY
CLASS="SECT1"
><DIV
CLASS="NAVHEADER"
><TABLE
SUMMARY="Header navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="4"
ALIGN="center"
VALIGN="bottom"
><A
HREF="index.html"
>pgpool-II 4.3devel Documentation</A
></TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
TITLE="Basic Configuration Example"
HREF="example-basic.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="60%"
ALIGN="center"
VALIGN="bottom"
>Chapter 8. Configuration Examples</TD
><TD
WIDTH="20%"
ALIGN="right"
VALIGN="top"
><A
TITLE="AWS Configuration Example"
HREF="example-aws.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="EXAMPLE-CLUSTER"
>8.2. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> + Watchdog Setup Example</A
></H1
><P
>  This section shows an example of streaming replication configuration using
  <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. In this example, we use 3
  <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers to manage <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>
  servers to create a robust cluster system and avoid the single point of failure or split brain.
 </P
><P
>  <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 13 is used in this configuration example.
  All scripts have been tested with <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 95 and later.
 </P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-REQUIREMENT"
>8.2.1. Requirements</A
></H2
><P
>   We assume that all the Pgpool-II servers and the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers are in the same subnet.
  </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-STRUCTURE"
>8.2.2. Cluster System Configuration</A
></H2
><P
>   We use 3 servers with CentOS 7.4. Let these servers be <TT
CLASS="LITERAL"
>server1</TT
>,
   <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>. 
   We install <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> and <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> on each server.
  </P
><P
>   <DIV
CLASS="FIGURE"
><A
NAME="AEN6603"
></A
><P
><B
>Figure 8-1. Cluster System Configuration</B
></P
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG
SRC="cluster_40.gif"></P
></DIV
></DIV
>
  </P
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>    The roles of <TT
CLASS="LITERAL"
>Active</TT
>, <TT
CLASS="LITERAL"
>Standby</TT
>, <TT
CLASS="LITERAL"
>Primary</TT
>,
    <TT
CLASS="LITERAL"
>Standby</TT
> are not fixed and may be changed by further operations.
   </P
></BLOCKQUOTE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-IP"
></A
><P
><B
>Table 8-2. Hostname and IP address</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Hostname</TH
><TH
>IP Address</TH
><TH
>Virtual IP</TH
></TR
></THEAD
><TBODY
><TR
><TD
>server1</TD
><TD
>192.168.137.101</TD
><TD
ROWSPAN="3"
>192.168.137.150</TD
></TR
><TR
><TD
>server2</TD
><TD
>192.168.137.102</TD
></TR
><TR
><TD
>server3</TD
><TD
>192.168.137.103</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-POSTGRESQL-CONFIG"
></A
><P
><B
>Table 8-3. PostgreSQL version and Configuration</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Item</TH
><TH
>Value</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>PostgreSQL Version</TD
><TD
>13.0</TD
><TD
>-</TD
></TR
><TR
><TD
>port</TD
><TD
>5432</TD
><TD
>-</TD
></TR
><TR
><TD
>$PGDATA</TD
><TD
>/var/lib/pgsql/13/data</TD
><TD
>-</TD
></TR
><TR
><TD
>Archive mode</TD
><TD
>on</TD
><TD
>/var/lib/pgsql/archivedir</TD
></TR
><TR
><TD
>Replication Slots</TD
><TD
>Enable</TD
><TD
>-</TD
></TR
><TR
><TD
>Start automatically</TD
><TD
>Enable</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-PGPOOL-CONFIG"
></A
><P
><B
>Table 8-4. Pgpool-II version and Configuration</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Item</TH
><TH
>Value</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>Pgpool-II Version</TD
><TD
>4.2.0</TD
><TD
>-</TD
></TR
><TR
><TD
ROWSPAN="4"
>port</TD
><TD
>9999</TD
><TD
>Pgpool-II accepts connections</TD
></TR
><TR
><TD
>9898</TD
><TD
>PCP process accepts connections</TD
></TR
><TR
><TD
>9000</TD
><TD
>watchdog accepts connections</TD
></TR
><TR
><TD
>9694</TD
><TD
>UDP port for receiving Watchdog's heartbeat signal</TD
></TR
><TR
><TD
>Config file</TD
><TD
>/etc/pgpool-II/pgpool.conf</TD
><TD
>Pgpool-II config file</TD
></TR
><TR
><TD
>Pgpool-II start user</TD
><TD
>postgres (Pgpool-II 4.1 or later)</TD
><TD
>Pgpool-II 4.0 or before, the default startup user is root</TD
></TR
><TR
><TD
>Running mode</TD
><TD
>streaming replication mode</TD
><TD
>-</TD
></TR
><TR
><TD
>Watchdog</TD
><TD
>on</TD
><TD
>Life check method: heartbeat</TD
></TR
><TR
><TD
>Start automatically</TD
><TD
>Enable</TD
><TD
>-</TD
></TR
></TBODY
></TABLE
></DIV
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-TABLE-SAMPLE-SCRIPTS"
></A
><P
><B
>Table 8-5. Various sample scripts included in rpm package</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>Feature</TH
><TH
>Script</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
ROWSPAN="2"
>Failover</TD
><TD
><A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/failover.sh.sample;hb=refs/heads/master"
TARGET="_top"
>/etc/pgpool-II/failover.sh.sample</A
></TD
><TD
>Run by <A
HREF="runtime-config-failover.html#GUC-FAILOVER-COMMAND"
>failover_command</A
> to perform failover</TD
></TR
><TR
><TD
><A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/follow_primary.sh.sample;hb=refs/heads/master"
TARGET="_top"
>/etc/pgpool-II/follow_primary.sh.sample</A
></TD
><TD
>Run by <A
HREF="runtime-config-failover.html#GUC-FOLLOW-PRIMARY-COMMAND"
>follow_primary_command</A
> to synchronize the Standby with the new Primary after failover.</TD
></TR
><TR
><TD
ROWSPAN="2"
>Online recovery</TD
><TD
><A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/recovery_1st_stage.sample;hb=refs/heads/master"
TARGET="_top"
>/etc/pgpool-II/recovery_1st_stage.sample</A
></TD
><TD
>Run by <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-1ST-STAGE-COMMAND"
>recovery_1st_stage_command</A
> to recovery a Standby node</TD
></TR
><TR
><TD
><A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/pgpool_remote_start.sample;hb=refs/heads/master"
TARGET="_top"
>/etc/pgpool-II/pgpool_remote_start.sample</A
></TD
><TD
>Run after <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-1ST-STAGE-COMMAND"
>recovery_1st_stage_command</A
> to start the Standby node</TD
></TR
><TR
><TD
ROWSPAN="2"
>Watchdog</TD
><TD
><A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/escalation.sh.sample;hb=refs/heads/master"
TARGET="_top"
>/etc/pgpool-II/escalation.sh.sample</A
></TD
><TD
>Run by <A
HREF="runtime-watchdog-config.html#GUC-WD-ESCALATION-COMMAND"
>wd_escalation_command</A
> to switch the Active/Standby Pgpool-II safely</TD
></TR
></TBODY
></TABLE
></DIV
><P
>   The above scripts are included in the RPM package and can be customized as needed.
  </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-INSTALLATION"
>8.2.3. Installation</A
></H2
><P
>   In this example, we install <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.2 and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> 13.0 using RPM packages.
  </P
><P
>   Install <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> using <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> YUM repository.
  </P
><PRE
CLASS="PROGRAMLISTING"
># yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
# yum install -y postgresql13-server
  </PRE
><P
>   Install <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> by using Pgpool-II YUM repository.
  </P
><PRE
CLASS="PROGRAMLISTING"
># yum install -y http://www.pgpool.net/yum/rpms/4.1/redhat/rhel-7-x86_64/pgpool-II-release-4.1-2.noarch.rpm
# yum install -y pgpool-II-pg13-*
  </PRE
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PRE-SETUP"
>8.2.4. Before Starting</A
></H2
><P
>   Before you start the configuration process, please check the following prerequisites.
  </P
><P
></P
><UL
><LI
><P
>     Set up <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> streaming replication on the primary server.
     In this example, we use WAL archiving.
    </P
><P
>     First, we create the directory <TT
CLASS="FILENAME"
>/var/lib/pgsql/archivedir</TT
> to store
     <ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
> segments on all servers. In this example, only Primary node archives
     <ACRONYM
CLASS="ACRONYM"
>WAL</ACRONYM
> locally.
    </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ mkdir /var/lib/pgsql/archivedir
    </PRE
><P
>     Then we edit the configuration file <TT
CLASS="FILENAME"
>$PGDATA/postgresql.conf</TT
>
     on <TT
CLASS="LITERAL"
>server1</TT
> (primary) as follows. Enable <TT
CLASS="LITERAL"
>wal_log_hints</TT
>
     to use <TT
CLASS="LITERAL"
>pg_rewind</TT
>. 
     Since the Primary may become a Standby later, we set <TT
CLASS="VARNAME"
>hot_standby = on</TT
>.
    </P
><PRE
CLASS="PROGRAMLISTING"
>listen_addresses = '*'
archive_mode = on
archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
max_wal_senders = 10
max_replication_slots = 10
wal_level = replica
hot_standby = on
wal_log_hints = on
    </PRE
><P
>     We use the online recovery functionality of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to setup standby server after the primary server is started.
    </P
></LI
><LI
><P
>     Because of the security reasons, we create a user <TT
CLASS="LITERAL"
>repl</TT
> solely used
     for replication purpose, and a user <TT
CLASS="LITERAL"
>pgpool</TT
> for streaming
     replication delay check and health check of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>.
    </P
><DIV
CLASS="TABLE"
><A
NAME="EXAMPLE-CLUSTER-USER"
></A
><P
><B
>Table 8-6. Users</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL><COL><COL><THEAD
><TR
><TH
>User Name</TH
><TH
>Password</TH
><TH
>Detail</TH
></TR
></THEAD
><TBODY
><TR
><TD
>repl</TD
><TD
>repl</TD
><TD
>PostgreSQL replication user</TD
></TR
><TR
><TD
>pgpool</TD
><TD
>pgpool</TD
><TD
>Pgpool-II health check (<A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-USER"
>health_check_user</A
>) and replication delay check (<A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
>) user</TD
></TR
><TR
><TD
>postgres</TD
><TD
>postgres</TD
><TD
>User running online recovery</TD
></TR
></TBODY
></TABLE
></DIV
><PRE
CLASS="PROGRAMLISTING"
>[server1]# psql -U postgres -p 5432
postgres=# SET password_encryption = 'scram-sha-256';
postgres=# CREATE ROLE pgpool WITH LOGIN;
postgres=# CREATE ROLE repl WITH REPLICATION LOGIN;
postgres=# \password pgpool
postgres=# \password repl
postgres=# \password postgres
    </PRE
><P
>     If you want to show "replication_state" and "replication_sync_state" column in
     <A
HREF="sql-show-pool-nodes.html"
>SHOW POOL NODES</A
> command result, role <TT
CLASS="LITERAL"
>pgpool</TT
>
      needs to be PostgreSQL super user or or in <TT
CLASS="LITERAL"
>pg_monitor</TT
> group 
      (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 or later). Grant <TT
CLASS="LITERAL"
>pg_monitor</TT
>
      to <TT
CLASS="LITERAL"
>pgpool</TT
>:
    </P
><PRE
CLASS="PROGRAMLISTING"
>GRANT pg_monitor TO pgpool;
    </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>      If you plan to use <A
HREF="runtime-config-failover.html#GUC-DETACH-FALSE-PRIMARY"
>detach_false_primary</A
>(<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 or later),
       role "pgpool" needs to be <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> super user or
       or in "pg_monitor" group to use this feature.
     </P
></BLOCKQUOTE
></DIV
><P
>     Assuming that all the <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> servers and the 
     <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers are in the same subnet and edit <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> to 
     enable <TT
CLASS="LITERAL"
>scram-sha-256</TT
> authentication method.
    </P
><PRE
CLASS="PROGRAMLISTING"
>host    all             all             samenet                 scram-sha-256
host    replication     all             samenet                 scram-sha-256
    </PRE
></LI
><LI
><P
>     To use the automated failover and online recovery of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, 
     the settings that allow <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>passwordless</I
></SPAN
> SSH to all backend servers
     between <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> execution user (default root user)
     and <TT
CLASS="LITERAL"
>postgres</TT
> user and between <TT
CLASS="LITERAL"
>postgres</TT
> user
     and <TT
CLASS="LITERAL"
>postgres</TT
> user are necessary. Execute the following command on all servers
     to set up passwordless <TT
CLASS="LITERAL"
>SSH</TT
>. The generated key file name is <TT
CLASS="LITERAL"
>id_rsa_pgpool</TT
>.
    </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# cd ~/.ssh
[all servers]# ssh-keygen -t rsa -f id_rsa_pgpool
[all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
[all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
[all servers]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3

[all servers]# su - postgres
[all servers]$ cd ~/.ssh
[all servers]$ ssh-keygen -t rsa -f id_rsa_pgpool
[all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
[all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
[all servers]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server3
    </PRE
><P
>     After setting, use <TT
CLASS="COMMAND"
>ssh postgres@serverX -i ~/.ssh/id_rsa_pgpool</TT
> command to
     make sure that you can log in without entering a password. Edit <TT
CLASS="FILENAME"
>/etc/ssh/sshd_config</TT
>
     if necessary and restart sshd.
    </P
></LI
><LI
><P
>     To allow <TT
CLASS="LITERAL"
>repl</TT
> user without specifying password for streaming 
     replication and online recovery, and execute <SPAN
CLASS="APPLICATION"
>pg_rewind</SPAN
>
     using <TT
CLASS="LITERAL"
>postgres</TT
>, we create the <TT
CLASS="FILENAME"
>.pgpass</TT
> file 
     in <TT
CLASS="LITERAL"
>postgres</TT
> user's home directory and change the permission to
     <TT
CLASS="LITERAL"
>600</TT
> on each <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server.
    </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ vi /var/lib/pgsql/.pgpass
server1:5432:replication:repl:&lt;repl user password&gt;
server2:5432:replication:repl:&lt;repl user passowrd&gt;
server3:5432:replication:repl:&lt;repl user passowrd&gt;
server1:5432:postgres:postgres:&lt;postgres user passowrd&gt;
server2:5432:postgres:postgres:&lt;postgres user passowrd&gt;
server3:5432:postgres:postgres:&lt;postgres user passowrd&gt;
[all servers]$ chmod 600  /var/lib/pgsql/.pgpass
    </PRE
></LI
><LI
><P
>     When connect to <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers, the target port must be accessible by enabling firewall management softwares. Following is an example for <SPAN
CLASS="SYSTEMITEM"
>CentOS/RHEL7</SPAN
>.
    </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# firewall-cmd --permanent --zone=public --add-service=postgresql
[all servers]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/udp
[all servers]# firewall-cmd --reload
    </PRE
></LI
><LI
><P
>     We set <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to start automatically on all servers.
    </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# systemctl enable pgpool.service
    </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>      If you set the auto-start of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, you need to change the <A
HREF="runtime-config-failover.html#GUC-SEARCH-PRIMARY-NODE-TIMEOUT"
>search_primary_node_timeout</A
> to an appropriate value that you can start the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> after the server has been started.
      <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> will fail if it can't connect to the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on the backend during the <TT
CLASS="LITERAL"
>search_primary_node_timeout</TT
>.
     </P
></BLOCKQUOTE
></DIV
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-NODE-ID"
>8.2.5. Create pgpool_node_id</A
></H2
><P
>    From <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.2, now all configuration parameters are identical on all hosts.
    If <TT
CLASS="LITERAL"
>watchdog</TT
> feature is enabled, to distinguish which host is which,
    a <TT
CLASS="FILENAME"
>pgpool_node_id</TT
> file is required.
    You need to create a <TT
CLASS="FILENAME"
>pgpool_node_id</TT
> file and specify the pgpool (watchdog) node number
    (e.g. 0, 1, 2 ...) to identify pgpool (watchdog) host.
  </P
><P
></P
><UL
><LI
><P
>     <TT
CLASS="LITERAL"
>server1</TT
>
    </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# cat /etc/pgpool-II/pgpool_node_id
0
    </PRE
></LI
><LI
><P
>     <TT
CLASS="LITERAL"
>server2</TT
>
    </P
><PRE
CLASS="PROGRAMLISTING"
>[server2]# cat /etc/pgpool-II/pgpool_node_id
1
    </PRE
></LI
><LI
><P
>     <TT
CLASS="LITERAL"
>server3</TT
>
    </P
><PRE
CLASS="PROGRAMLISTING"
>[server3]# cat /etc/pgpool-II/pgpool_node_id
2
    </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG"
>8.2.6. <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> Configuration</A
></H2
><P
>   Since from <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.2, all configuration parameters are
   identical on all hosts, you can edit <TT
CLASS="FILENAME"
>pgpool.conf</TT
> on any pgpool node
   and copy the edited <TT
CLASS="FILENAME"
>pgpool.conf</TT
> file to the other pgpool nodes.
  </P
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-CONFIG-FILE"
>8.2.6.1. Clustering mode</A
></H3
><P
>    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> has several clustering modes. To set the clustering
    mode, <A
HREF="runtime-config-running-mode.html#GUC-BACKEND-CLUSTERING-MODE"
>backend_clustering_mode</A
> can be used. In this configuration
    example, streaming replication mode is used.
   </P
><P
>    When installing <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> using RPM, all the
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> configuration sample files are in <TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>.
    In this example, we copy the sample configuration file for streaming replication mode.
   </P
><PRE
CLASS="PROGRAMLISTING"
># cp -p /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-LISTEN-ADDRESSES"
>8.2.6.2. listen_addresses</A
></H3
><P
>    To allow Pgpool-II to accept all incoming connections, we set <TT
CLASS="VARNAME"
>listen_addresses = '*'</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>listen_addresses = '*'
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-PORT"
>8.2.6.3. port</A
></H3
><P
>    Specify the port number Pgpool-II listen on.
   </P
><PRE
CLASS="PROGRAMLISTING"
>port = 9999
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-SR-CHECK"
>8.2.6.4. Streaming Replication Check</A
></H3
><P
>    Specify replication delay check user and password in <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
>
    and <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>. In this example, we leave
    <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
> empty, and create the entry in
    <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>. See <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PGPOOL-CONFIG-AUTH"
>Section 8.2.6.9</A
>
    for how to create the entry in <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>.
    From <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0, if these parameters are left blank,
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> will first try to get the password for that
    specific user from <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
> file before using the empty password.
   </P
><PRE
CLASS="PROGRAMLISTING"
>sr_check_user = 'pgpool'
sr_check_password = ''
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-HEALTH-CHECK"
>8.2.6.5. Health Check</A
></H3
><P
>    Enable health check so that <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> performs failover. Also, if the network is unstable,
    the health check fails even though the backend is running properly, failover or degenerate operation may occur.
    In order to prevent such incorrect detection of health check, we set <TT
CLASS="VARNAME"
>health_check_max_retries = 3</TT
>.
    Specify <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-USER"
>health_check_user</A
> and <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
> in
      the same way like <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-USER"
>sr_check_user</A
> and <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>health_check_period = 5
                                            # Health check period
                                            # Disabled (0) by default
health_check_timeout = 30
                                            # Health check timeout
                                            # 0 means no timeout
health_check_user = 'pgpool'
health_check_password = ''

health_check_max_retries = 3
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-BACKEND-SETTINGS"
>8.2.6.6. Backend Settings</A
></H3
><P
>    Specify the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> backend information.
    Multiple backends can be specified by adding a number at the end of the parameter name.
   </P
><PRE
CLASS="PROGRAMLISTING"
># - Backend Connection Settings -

backend_hostname0 = 'server1'
                                            # Host name or IP address to connect to for backend 0
backend_port0 = 5432
                                            # Port number for backend 0
backend_weight0 = 1
                                            # Weight for backend 0 (only in load balancing mode)
backend_data_directory0 = '/var/lib/pgsql/13/data'
                                            # Data directory for backend 0
backend_flag0 = 'ALLOW_TO_FAILOVER'
                                            # Controls various backend behavior
                                            # ALLOW_TO_FAILOVER or DISALLOW_TO_FAILOVER
backend_hostname1 = 'server2'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/var/lib/pgsql/13/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'

backend_hostname2 = 'server3'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/var/lib/pgsql/13/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'
   </PRE
><P
>    To show "replication_state" and "replication_sync_state" column in <A
HREF="sql-show-pool-nodes.html"
>SHOW POOL NODES</A
>
     command result, <A
HREF="runtime-config-backend-settings.html#GUC-BACKEND-APPLICATION-NAME"
>backend_application_name</A
> parameter is required.
      Here we specify each backend's hostname in these parameters. (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 or later)
   </P
><PRE
CLASS="PROGRAMLISTING"
>...
backend_application_name0 = 'server1'
...
backend_application_name1 = 'server2'
...
backend_application_name2 = 'server3'
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-FAILOVER"
>8.2.6.7. Failover configuration</A
></H3
><P
>    Specify failover.sh script to be executed after failover in <TT
CLASS="VARNAME"
>failover_command</TT
>
    parameter. 
    If we use 3 PostgreSQL servers, we need to specify follow_primary_command to run after failover on the primary node failover.
    In case of two PostgreSQL servers, follow_primary_command setting is not necessary.
   </P
><P
>    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> replaces the following special characters with the backend specific
    information while executing the scripts. 
    See <A
HREF="runtime-config-failover.html#GUC-FAILOVER-COMMAND"
>failover_command</A
> for more details about each character.
   </P
><PRE
CLASS="PROGRAMLISTING"
>failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
follow_primary_command = '/etc/pgpool-II/follow_primary.sh %d %h %p %D %m %H %M %P %r %R'
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>%N</I
></SPAN
> and <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>%S</I
></SPAN
> are added in <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1.
     Please note that these characters cannot be specified if using Pgpool-II 4.0 or earlier.
    </P
></BLOCKQUOTE
></DIV
><P
>    Sample scripts <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/failover.sh.sample;hb=refs/heads/master"
TARGET="_top"
>failover.sh</A
>
    and <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/follow_primary.sh.sample;hb=refs/heads/master"
TARGET="_top"
>follow_primary.sh</A
>
    are installed in <TT
CLASS="FILENAME"
>/etc/pgpool-II/</TT
>. Create failover scripts using these sample files.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# cp -p /etc/pgpool-II/failover.sh{.sample,}
[all servers]# cp -p /etc/pgpool-II/follow_primary.sh{.sample,}
[all servers]# chown postgres:postgres /etc/pgpool-II/{failover.sh,follow_primary.sh}
   </PRE
><P
>    Basically, it should work if you change <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>PGHOME</I
></SPAN
> according to PostgreSQL installation directory.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# vi /etc/pgpool-II/failover.sh
...
PGHOME=/usr/pgsql-13
...

[all servers]# vi /etc/pgpool-II/follow_primary.sh
...
PGHOME=/usr/pgsql-13
...
   </PRE
><P
>    Since user authentication is required to use the <TT
CLASS="LITERAL"
>PCP</TT
> command in
    <TT
CLASS="VARNAME"
>follow_primary_command</TT
> script,
    we need to specify user name and md5 encrypted password in <TT
CLASS="FILENAME"
>pcp.conf</TT
>
    in format "<TT
CLASS="LITERAL"
>username:encrypted password</TT
>".
   </P
><P
>    if <TT
CLASS="LITERAL"
>pgpool</TT
> user is specified in <TT
CLASS="VARNAME"
>PCP_USER</TT
> in <TT
CLASS="FILENAME"
>follow_primary.sh</TT
>,
   </P
><PRE
CLASS="PROGRAMLISTING"
># cat /etc/pgpool-II/follow_primary.sh
...
PCP_USER=pgpool
...
   </PRE
><P
>    then we use <A
HREF="pg-md5.html"
>pg_md5</A
> to create the encrypted password entry for <TT
CLASS="LITERAL"
>pgpool</TT
> user as below:
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# echo 'pgpool:'`pg_md5 PCP password` &gt;&gt; /etc/pgpool-II/pcp.conf
   </PRE
><P
>    Since <TT
CLASS="FILENAME"
>follow_primary.sh</TT
> script must execute PCP command without entering a
    password, we need to create <TT
CLASS="FILENAME"
>.pcppass</TT
> in the home directory of
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> startup user (postgres user) on each server.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ echo 'localhost:9898:pgpool:&lt;pgpool user password&gt;' &gt; ~/.pcppass
[all servers]$ chmod 600 ~/.pcppass
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     The <TT
CLASS="FILENAME"
>follow_primary.sh</TT
> script does not support tablespaces.
     If you are using tablespaces, you need to modify the script to support tablespaces.
    </P
></BLOCKQUOTE
></DIV
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-ONLINE-RECOVERY"
>8.2.6.8. Pgpool-II Online Recovery Configurations</A
></H3
><P
>    Next, in order to perform online recovery with <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> we specify
    the <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> user name and online recovery command 
    <TT
CLASS="COMMAND"
>recovery_1st_stage</TT
>.
    Because <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>Superuser</I
></SPAN
> privilege in <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>
    is required for performing online recovery, we specify <TT
CLASS="LITERAL"
>postgres</TT
> user in <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-USER"
>recovery_user</A
>.
     Then, we create <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
>
     in database cluster directory of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> primary server (server1), and add execute permission.

   </P
><PRE
CLASS="PROGRAMLISTING"
>recovery_user = 'postgres'
                                            # Online recovery user
recovery_password = ''
                                            # Online recovery password

recovery_1st_stage_command = 'recovery_1st_stage'
   </PRE
><P
>    Online recovery sample scripts<A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/recovery_1st_stage.sample;hb=refs/heads/master"
TARGET="_top"
>recovery_1st_stage</A
>
    and <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/pgpool_remote_start.sample;hb=refs/heads/master"
TARGET="_top"
>pgpool_remote_start</A
>
    are installed in <TT
CLASS="FILENAME"
>/etc/pgpool-II/</TT
>. Copy these files to the data directory of the primary server (server1).
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# cp -p /etc/pgpool-II/recovery_1st_stage.sample /var/lib/pgsql/13/data/recovery_1st_stage
[server1]# cp -p /etc/pgpool-II/pgpool_remote_start.sample /var/lib/pgsql/13/data/pgpool_remote_start
[server1]# chown postgres:postgres /var/lib/pgsql/13/data/{recovery_1st_stage,pgpool_remote_start}
   </PRE
><P
>    Basically, it should work if you change <SPAN
CLASS="emphasis"
><I
CLASS="EMPHASIS"
>PGHOME</I
></SPAN
> according to PostgreSQL installation directory.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# vi /var/lib/pgsql/13/data/recovery_1st_stage
...
PGHOME=/usr/pgsql-13
...

[server1]# vi /var/lib/pgsql/13/data/pgpool_remote_start
...
PGHOME=/usr/pgsql-13
...
   </PRE
><P
>    In order to use the online recovery functionality, the functions of
    <CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
>, <CODE
CLASS="FUNCTION"
>pgpool_remote_start</CODE
>,
    <CODE
CLASS="FUNCTION"
>pgpool_switch_xlog</CODE
> are required, so we need install
    <CODE
CLASS="FUNCTION"
>pgpool_recovery</CODE
> on template1 of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server
    <TT
CLASS="LITERAL"
>server1</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# su - postgres
[server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     The <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> script does not support tablespaces.
     If you are using tablespaces, you need to modify the script to support tablespaces.
    </P
></BLOCKQUOTE
></DIV
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-AUTH"
>8.2.6.9. Client Authentication Configuration</A
></H3
><P
>    Because in the section <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>Before Starting</A
>,
    we already set <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> authentication method to
    <ACRONYM
CLASS="ACRONYM"
>scram-sha-256</ACRONYM
>, it is necessary to set a client authentication by
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> to connect to backend nodes.
    When installing with RPM, the <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> configuration file
    <TT
CLASS="FILENAME"
>pool_hba.conf</TT
> is in <TT
CLASS="FILENAME"
>/etc/pgpool-II</TT
>.
    By default, pool_hba authentication is disabled, set <TT
CLASS="VARNAME"
>enable_pool_hba = on</TT
>
    to enable it.
   </P
><PRE
CLASS="PROGRAMLISTING"
>enable_pool_hba = on
   </PRE
><P
>    The format of <TT
CLASS="FILENAME"
>pool_hba.conf</TT
> file follows very closely PostgreSQL's 
    <TT
CLASS="FILENAME"
>pg_hba.conf</TT
> format. Set <TT
CLASS="LITERAL"
>pgpool</TT
> and <TT
CLASS="LITERAL"
>postgres</TT
> user's authentication method to <TT
CLASS="LITERAL"
>scram-sha-256</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>host    all         pgpool           0.0.0.0/0          scram-sha-256
host    all         postgres         0.0.0.0/0          scram-sha-256
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     Please note that in <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 only AES encrypted password or clear text password
     can be specified in <A
HREF="runtime-config-health-check.html#GUC-HEALTH-CHECK-PASSWORD"
>health_check_password</A
>, <A
HREF="runtime-streaming-replication-check.html#GUC-SR-CHECK-PASSWORD"
>sr_check_password</A
>, 
       <A
HREF="runtime-watchdog-config.html#GUC-WD-LIFECHECK-PASSWORD"
>wd_lifecheck_password</A
>, <A
HREF="runtime-online-recovery.html#GUC-RECOVERY-PASSWORD"
>recovery_password</A
> in <TT
CLASS="FILENAME"
>pgpool.conf</TT
>.
    </P
></BLOCKQUOTE
></DIV
><P
>    The default password file name for authentication is <A
HREF="runtime-config-connection.html#GUC-POOL-PASSWD"
>pool_passwd</A
>.
     To use <TT
CLASS="LITERAL"
>scram-sha-256</TT
> authentication, the decryption key to decrypt the passwords
     is required. We create the <TT
CLASS="LITERAL"
>.pgpoolkey</TT
> file in <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
     start user <TT
CLASS="LITERAL"
>postgres</TT
>'s (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.1 or later) home directory.
     (<SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 4.0 or before, by default <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
     is started as <TT
CLASS="LITERAL"
>root</TT
>)
     </P><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ echo 'some string' &#62; ~/.pgpoolkey
[all servers]$ chmod 600 ~/.pgpoolkey
     </PRE
><P>
   </P
><P
>    Execute command <TT
CLASS="COMMAND"
>pg_enc -m -k /path/to/.pgpoolkey -u username -p</TT
> to register user
    name and <TT
CLASS="LITERAL"
>AES</TT
> encrypted password in file <TT
CLASS="FILENAME"
>pool_passwd</TT
>.
    If <TT
CLASS="FILENAME"
>pool_passwd</TT
> doesn't exist yet, it will be created in the same directory as
    <TT
CLASS="FILENAME"
>pgpool.conf</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# su - postgres
[all servers]$ pg_enc -m -k ~/.pgpoolkey -u pgpool -p
db password: [pgpool user's password]
[all servers]$ pg_enc -m -k ~/.pgpoolkey -u postgres -p
db password: [postgres user's password]

# cat /etc/pgpool-II/pool_passwd
pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
postgres:AESHs/pWL5rtXy2IwuzroHfqg==
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-WATCHDOG"
>8.2.6.10. Watchdog Configuration</A
></H3
><P
>    Enable watchdog functionality on <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>use_watchdog = on
   </PRE
><P
>    Specify virtual IP address that accepts connections from clients on 
    <TT
CLASS="LITERAL"
>server1</TT
>, <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
>. 
    Ensure that the IP address set to virtual IP isn't used yet.
   </P
><PRE
CLASS="PROGRAMLISTING"
>delegate_IP = '192.168.137.150'
   </PRE
><P
>    To bring up/down the virtual IP and send the ARP requests, we set <A
HREF="runtime-watchdog-config.html#GUC-IF-UP-CMD"
>if_up_cmd</A
>, <A
HREF="runtime-watchdog-config.html#GUC-IF-DOWN-CMD"
>if_down_cmd</A
> and <A
HREF="runtime-watchdog-config.html#GUC-ARPING-CMD"
>arping_cmd</A
>.
    The network interface used in this example is "enp0s8".
    Since root privilege is required to execute <TT
CLASS="VARNAME"
>if_up/down_cmd</TT
> or
    <TT
CLASS="VARNAME"
>arping_cmd</TT
> command, use setuid on these command or allow
    <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> startup user, <TT
CLASS="LITERAL"
>postgres</TT
> user (Pgpool-II 4.1 or later) to run <TT
CLASS="COMMAND"
>sudo</TT
> command without a password.
   </P
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>    If <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> is installed using RPM, the <TT
CLASS="LITERAL"
>postgres</TT
>
    user has been configured to run <TT
CLASS="COMMAND"
>ip/arping</TT
> via <TT
CLASS="COMMAND"
>sudo</TT
> without
    a password.
    </P><PRE
CLASS="PROGRAMLISTING"
>postgres ALL=NOPASSWD: /sbin/ip
postgres ALL=NOPASSWD: /usr/sbin/arping
    </PRE
><P>
    </P
></BLOCKQUOTE
></DIV
><P
>    Here we configure the following parameters to run <TT
CLASS="VARNAME"
>if_up/down_cmd</TT
> or <TT
CLASS="VARNAME"
>arping_cmd</TT
> with sudo.
   </P
><PRE
CLASS="PROGRAMLISTING"
>if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev enp0s8'
arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I enp0s8'
   </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     If "Defaults requiretty" is set in the <TT
CLASS="FILENAME"
>/etc/sudoers</TT
>,
     please ensure that the <SPAN
CLASS="PRODUCTNAME"
>pgpool</SPAN
> startup user can execute the <TT
CLASS="COMMAND"
>if_up_cmd</TT
>, <TT
CLASS="COMMAND"
>if_down_cmd</TT
> and <TT
CLASS="COMMAND"
>arping_cmd</TT
> command without a tty.
    </P
></BLOCKQUOTE
></DIV
><P
>    Set <A
HREF="runtime-watchdog-config.html#GUC-IF-CMD-PATH"
>if_cmd_path</A
> and <A
HREF="runtime-watchdog-config.html#GUC-ARPING-PATH"
>arping_path</A
> according to the
    command path.
    If <TT
CLASS="VARNAME"
>if_up/down_cmd</TT
> or <TT
CLASS="VARNAME"
>arping_cmd</TT
> starts with "/", these parameters will be ignored. 
   </P
><PRE
CLASS="PROGRAMLISTING"
>if_cmd_path = '/sbin'
arping_path = '/usr/sbin'
   </PRE
><P
>    Specify all <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> nodes information for configuring watchdog.
    Specify <TT
CLASS="VARNAME"
>pgpool_portX</TT
> using the port number specified in <TT
CLASS="VARNAME"
>port</TT
> in
    <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PGPOOL-CONFIG-PORT"
>Section 8.2.6.3</A
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>hostname0 = 'server1'
                                    # Host name or IP address of pgpool node
                                    # for watchdog connection
                                    # (change requires restart)
wd_port0 = 9000
                                    # Port number for watchdog service
                                    # (change requires restart)
pgpool_port0 = 9999
                                    # Port number for pgpool
                                    # (change requires restart)

hostname1 = 'server2'
wd_port1 = 9000
pgpool_port1 = 9999

hostname2 = 'server3'
wd_port2 = 9000
pgpool_port2 = 9999
   </PRE
><P
>    Specify the method of lifecheck <A
HREF="runtime-watchdog-config.html#GUC-WD-LIFECHECK-METHOD"
>wd_lifecheck_method</A
>
    and the lifecheck interval <A
HREF="runtime-watchdog-config.html#GUC-WD-INTERVAL"
>wd_interval</A
>.
    Here, we use <TT
CLASS="LITERAL"
>heartbeat</TT
> method to perform watchdog lifecheck.
   </P
><PRE
CLASS="PROGRAMLISTING"
>wd_lifecheck_method = 'heartbeat'
                                    # Method of watchdog lifecheck ('heartbeat' or 'query' or 'external')
                                    # (change requires restart)
wd_interval = 10
                                    # lifecheck interval (sec) &#62; 0
                                    # (change requires restart)
   </PRE
><P
>    Specify all <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> nodes information for sending and receiving heartbeat signal.
   </P
><PRE
CLASS="PROGRAMLISTING"
>heartbeat_hostname0 = 'server1'
                                    # Host name or IP address used
                                    # for sending heartbeat signal.
                                    # (change requires restart)
heartbeat_port0 = 9694
                                    # Port number used for receiving/sending heartbeat signal
                                    # Usually this is the same as heartbeat_portX.
                                    # (change requires restart)
heartbeat_device0 = ''
                                    # Name of NIC device (such like 'eth0')
                                    # used for sending/receiving heartbeat
                                    # signal to/from destination 0.
                                    # This works only when this is not empty
                                    # and pgpool has root privilege.
                                    # (change requires restart)

heartbeat_hostname1 = 'server2'
heartbeat_port1 = 9694
heartbeat_device1 = ''
heartbeat_hostname2 = 'server3'
heartbeat_port2 = 9694
heartbeat_device2 = ''
   </PRE
><P
>    If the <A
HREF="runtime-watchdog-config.html#GUC-WD-LIFECHECK-METHOD"
>wd_lifecheck_method</A
> is set to <TT
CLASS="LITERAL"
>heartbeat</TT
>,
    specify the time to detect a fault <A
HREF="runtime-watchdog-config.html#GUC-WD-HEARTBEAT-DEADTIME"
>wd_heartbeat_deadtime</A
> and
    the interval to send heartbeat signals <A
HREF="runtime-watchdog-config.html#GUC-WD-HEARTBEAT-DEADTIME"
>wd_heartbeat_deadtime</A
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>wd_heartbeat_keepalive = 2
                                    # Interval time of sending heartbeat signal (sec)
                                    # (change requires restart)
wd_heartbeat_deadtime = 30
                                    # Deadtime interval for heartbeat signal (sec)
                                    # (change requires restart)
   </PRE
><P
>     When <TT
CLASS="LITERAL"
>Watchdog</TT
> process is abnormally terminated, the virtual IP may be "up" on both of the old and new active pgpool nodes.
     To prevent this, configure <A
HREF="runtime-watchdog-config.html#GUC-WD-ESCALATION-COMMAND"
>wd_escalation_command</A
> to bring down the virtual IP on other pgpool nodes before bringing up the virtual IP on the new active pgpool node.
   </P
><PRE
CLASS="PROGRAMLISTING"
>wd_escalation_command = '/etc/pgpool-II/escalation.sh'
                                    # Executes this command at escalation on new active pgpool.
                                    # (change requires restart)
    </PRE
><P
>    The sample script <A
HREF="https://git.postgresql.org/gitweb/?p=pgpool2.git;a=blob_plain;f=src/sample/scripts/escalation.sh.sample;hb=refs/heads/master"
TARGET="_top"
>escalation.sh</A
> is installed in <TT
CLASS="FILENAME"
>/etc/pgpool-II/</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# cp -p /etc/pgpool-II/escalation.sh{.sample,}
[all servers]# chown postgres:postgres /etc/pgpool-II/escalation.sh
    </PRE
><P
>    Basically, it should work if you change the following variables according to your environment.
    PGPOOL is tha array of the hostname that running Pgpool-II.
    VIP is the virtual IP address that you set as delegate_IP.
    DEVICE is the network interface for the virtual IP.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# vi /etc/pgpool-II/escalation.sh
...
PGPOOLS=(server1 server2 server3)
VIP=192.168.137.150
DEVICE=enp0s8
...
    </PRE
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     If you have even number of watchdog nodes, you need to turn on <A
HREF="runtime-watchdog-config.html#GUC-ENABLE-CONSENSUS-WITH-HALF-VOTES"
>enable_consensus_with_half_votes</A
> parameter.
    </P
></BLOCKQUOTE
></DIV
><DIV
CLASS="NOTE"
><BLOCKQUOTE
CLASS="NOTE"
><P
><B
>Note: </B
>     If use_watchdog = on, please make sure the pgpool node number is specified
     in <TT
CLASS="FILENAME"
>pgpool_node_id</TT
> file.
     See <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PGPOOL-NODE-ID"
>Section 8.2.5</A
> for details.
    </P
></BLOCKQUOTE
></DIV
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-LOG"
>8.2.6.11. Logging</A
></H3
><P
>    Since Pgpool-II 4.2, the logging collector process has been implemented.
    In the example, we enable logging collector.
   </P
><PRE
CLASS="PROGRAMLISTING"
>log_destination = 'stderr'
logging_collector = on
log_directory = '/var/log/pgpool_log'
log_filename = 'pgpool-%Y-%m-%d_%H%M%S.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 10MB
   </PRE
><P
>    Create the log directory on all servers.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# mkdir /var/log/pgpool_log/
[all servers]# chown postgres:postgres /var/log/pgpool_log/
   </PRE
><P
>   The configuration of <TT
CLASS="FILENAME"
>pgpool.conf</TT
> on server1 is completed. Copy the <TT
CLASS="FILENAME"
>pgpool.conf</TT
>
   to other <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> nodes (server2 and server3).
  </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# scp -p /etc/pgpool-II/pgpool.conf root@server2:/etc/pgpool-II/pgpool.conf
[server1]# scp -p /etc/pgpool-II/pgpool.conf root@server3:/etc/pgpool-II/pgpool.conf
  </PRE
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-PGPOOL-CONFIG-SYSCONFIG"
>8.2.7. /etc/sysconfig/pgpool Configuration</A
></H2
><P
>   When starting <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, if the <TT
CLASS="FILENAME"
>pgpool_status</TT
>
   file exists, <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> will read the backend status (up/down) from the
   <TT
CLASS="FILENAME"
>pgpool_status</TT
> file.
  </P
><P
>   If you want to ignore the <TT
CLASS="FILENAME"
>pgpool_status</TT
> file at startup of
   <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, add "- D" to the start option OPTS to
   <TT
CLASS="FILENAME"
>/etc/sysconfig/pgpool</TT
>.
  </P
><PRE
CLASS="PROGRAMLISTING"
>[all servers]# vi /etc/sysconfig/pgpool
...
OPTS=" -D -n"
  </PRE
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-START-STOP"
>8.2.8. Starting/Stopping Pgpool-II</A
></H2
><P
>   Next we start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. Before starting 
   <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, please start 
   <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> servers first. 
   Also, when stopping <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
>, it is necessary to 
   stop Pgpool-II first.
  </P
><P
></P
><UL
><LI
><P
>     Starting <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
    </P
><P
>     In section <A
HREF="example-cluster.html#EXAMPLE-CLUSTER-PRE-SETUP"
>Before Starting</A
>,
     we already set the auto-start of <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. To start
     <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>, restart the whole system or execute the following command.
    </P
><PRE
CLASS="PROGRAMLISTING"
># systemctl start pgpool.service
    </PRE
></LI
><LI
><P
>     Stopping <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>
    </P
><PRE
CLASS="PROGRAMLISTING"
># systemctl stop pgpool.service
    </PRE
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="EXAMPLE-CLUSTER-TRY"
>8.2.9. How to use</A
></H2
><P
>   Let's start to use <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
>. 
   First, let's start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> on <TT
CLASS="LITERAL"
>server1</TT
>, 
   <TT
CLASS="LITERAL"
>server2</TT
>, <TT
CLASS="LITERAL"
>server3</TT
> by using the following command.
  </P
><PRE
CLASS="PROGRAMLISTING"
># systemctl start pgpool.service
  </PRE
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-STANDBY"
>8.2.9.1. Set up PostgreSQL standby server</A
></H3
><P
>    First, we should set up <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> standby server by
    using <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> online recovery functionality. Ensure
    that <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
>
    scripts used by <TT
CLASS="COMMAND"
>pcp_recovery_node</TT
> command are in database
    cluster directory of <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> primary server (<TT
CLASS="LITERAL"
>server1</TT
>).
   </P
><PRE
CLASS="PROGRAMLISTING"
># pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 1
Password:
pcp_recovery_node -- Command Successful

# pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 2
Password:
pcp_recovery_node -- Command Successful
   </PRE
><P
>    After executing <TT
CLASS="COMMAND"
>pcp_recovery_node</TT
> command,
    verify that <TT
CLASS="LITERAL"
>server2</TT
> and <TT
CLASS="LITERAL"
>server3</TT
>
    are started as <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> standby server.
   </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool
node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
(3 rows)
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-WATCHDOG"
>8.2.9.2. Switching active/standby watchdog</A
></H3
><P
>    Confirm the watchdog status by using <TT
CLASS="COMMAND"
>pcp_watchdog_info</TT
>. The <TT
CLASS="COMMAND"
>Pgpool-II</TT
> server which is started first run as <TT
CLASS="LITERAL"
>LEADER</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
># pcp_watchdog_info -h 192.168.137.150 -p 9898 -U pgpool
Password:
3 YES server1:9999 Linux server1 server1

server1:9999 Linux server1 server1 9999 9000 4 LEADER  #The Pgpool-II server started first became "LEADER".
server2:9999 Linux server2 server2 9999 9000 7 STANDBY #run as standby
server3:9999 Linux server3 server3 9999 9000 7 STANDBY #run as standby
   </PRE
><P
>    Stop active server <TT
CLASS="LITERAL"
>server1</TT
>, then <TT
CLASS="LITERAL"
>server2</TT
> or 
    <TT
CLASS="LITERAL"
>server3</TT
> will be promoted to active server. To stop 
    <TT
CLASS="LITERAL"
>server1</TT
>, we can stop <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> 
    service or shutdown the whole system. Here, we stop <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> service.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# systemctl stop pgpool.service

# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
Password:
3 YES server2:9999 Linux server2 server2

server2:9999 Linux server2 server2 9999 9000 4 LEADER     #server2 is promoted to LEADER
server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN  #server1 is stopped
server3:9999 Linux server3 server3 9999 9000 7 STANDBY    #server3 runs as STANDBY
   </PRE
><P
>    Start <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> (<TT
CLASS="LITERAL"
>server1</TT
>) which we have stopped again,
    and verify that <TT
CLASS="LITERAL"
>server1</TT
> runs as a standby.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]# systemctl start pgpool.service

[server1]# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
Password: 
3 YES server2:9999 Linux server2 server2

server2:9999 Linux server2 server2 9999 9000 4 LEADER
server1:9999 Linux server1 server1 9999 9000 7 STANDBY
server3:9999 Linux server3 server3 9999 9000 7 STANDBY
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-FAILOVER"
>8.2.9.3. Failover</A
></H3
><P
>    First, use <TT
CLASS="COMMAND"
>psql</TT
> to connect to <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> via virtual IP,
    and verify the backend information.
   </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool:
node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
(3 rows)
   </PRE
><P
>    Next, stop primary <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> server 
    <TT
CLASS="LITERAL"
>server1</TT
>, and verify automatic failover.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server1]$ pg_ctl -D /var/lib/pgsql/13/data -m immediate stop
   </PRE
><P
>    After stopping <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on <TT
CLASS="LITERAL"
>server1</TT
>,
    failover occurs and <SPAN
CLASS="PRODUCTNAME"
>PostgreSQL</SPAN
> on 
    <TT
CLASS="LITERAL"
>server2</TT
> becomes new primary DB.
   </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool:
node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
0       | server1  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
1       | server2  | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 |                   |                        | 2019-08-06 11:36:03
2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:36:15
(3 rows)
   </PRE
><P
>    <TT
CLASS="LITERAL"
>server3</TT
> is running as standby of new primary <TT
CLASS="LITERAL"
>server2</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
>[server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
pg_is_in_recovery 
-------------------
t

[server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
pg_is_in_recovery 
-------------------
f

[server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select * from pg_stat_replication" -x
-[ RECORD 1 ]----+------------------------------
pid              | 11059
usesysid         | 16392
usename          | repl
application_name | server3
client_addr      | 192.168.137.103
client_hostname  | 
client_port      | 48694
backend_start    | 2019-08-06 11:36:07.479161+09
backend_xmin     | 
state            | streaming
sent_lsn         | 0/75000148
write_lsn        | 0/75000148
flush_lsn        | 0/75000148
replay_lsn       | 0/75000148
write_lag        | 
flush_lag        | 
replay_lag       | 
sync_priority    | 0
sync_state       | async
reply_time       | 2019-08-06 11:42:59.823961+09
   </PRE
></DIV
><DIV
CLASS="SECT3"
><H3
CLASS="SECT3"
><A
NAME="EXAMPLE-CLUSTER-TRY-ONLINE-RECOVERY"
>8.2.9.4. Online Recovery</A
></H3
><P
>    Here, we use <SPAN
CLASS="PRODUCTNAME"
>Pgpool-II</SPAN
> online recovery functionality to
    restore <TT
CLASS="LITERAL"
>server1</TT
> (old primary server) as a standby. Before 
    restoring the old primary server, please ensure that 
    <TT
CLASS="FILENAME"
>recovery_1st_stage</TT
> and <TT
CLASS="FILENAME"
>pgpool_remote_start</TT
> scripts 
    exist in database cluster directory of current primary server <TT
CLASS="LITERAL"
>server2</TT
>.
   </P
><PRE
CLASS="PROGRAMLISTING"
># pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 0
Password: 
pcp_recovery_node -- Command Successful
   </PRE
><P
>    Then verify that <TT
CLASS="LITERAL"
>server1</TT
> is started as a standby.
   </P
><PRE
CLASS="PROGRAMLISTING"
># psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
Password for user pgpool:
node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
0       | server1  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:48:05
1       | server2  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:36:15
(3 rows)
   </PRE
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
SUMMARY="Footer navigation table"
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="example-basic.html"
ACCESSKEY="P"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
ACCESSKEY="H"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="example-aws.html"
ACCESSKEY="N"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Basic Configuration Example</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="example-configs.html"
ACCESSKEY="U"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>AWS Configuration Example</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>